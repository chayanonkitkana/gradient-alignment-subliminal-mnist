{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"14DhCN01TrWezj-r_uHrjDt86GyI_dUvP","timestamp":1769796737425},{"file_id":"1OwMiMaoz-Gp47GmuL0k6HJqGQ7R9Tjt1","timestamp":1769597644845},{"file_id":"19ybyLca3ED_pG7MLNYxS_U1T-JHUgttR","timestamp":1769015223347}],"machine_shape":"hm","authorship_tag":"ABX9TyNKh0Y4XkqVzM3NTrbFFwJY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VPr_IBh8m29p","executionInfo":{"status":"ok","timestamp":1770191771349,"user_tz":-420,"elapsed":7730,"user":{"displayName":"Chayanon Kitkana","userId":"07549743553674769868"}},"outputId":"5569a48b-a37f-41b6-877d-5f6f4d17871e"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch: 2.9.0+cpu\n","torchvision: 0.24.0+cpu\n","device: cpu\n"]}],"source":["import os\n","import json\n","import time\n","import random\n","from dataclasses import dataclass, asdict\n","from pathlib import Path\n","from typing import Callable, Dict, List, Tuple, Optional\n","\n","import numpy as np\n","import pandas as pd\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader, random_split\n","\n","import torchvision\n","import torchvision.transforms as T\n","\n","print(\"torch:\", torch.__version__)\n","print(\"torchvision:\", torchvision.__version__)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"device:\", device)"]},{"cell_type":"code","source":["# Liminal-style anchoring (KL-to-base) in auxiliary-logit distillation\n","#\n","# Conditions:\n","# - **Control**: normal auxiliary-logit distillation (student matches teacher aux logits on noise).\n","# - **Intervention (liminal anchor)**: augment distillation with a KL regularizer that anchors the student\n","#   to a frozen **base model** (the shared initialization) on selected logits:\n","#       L_total = L_distill_aux + lambda_k * KL(p_base^T || p_student^T)\n","#   where the anchor is computed either on:\n","#       - \"reg10\": the first 10 (MNIST) logits, or\n","#       - \"all\":   all (10 + aux) logits.\n","#   The weight lambda_k can be scheduled (strong early then decay) or kept constant throughout training.\n","#\n","# Measurements (logged every N steps):\n","# - trait loss (CE on first 10 logits, on audit batches)\n","# - distill loss (KL on aux logits, on current noise batch)\n","# - anchor loss (KL-to-base on selected logits, on current noise batch) and lambda_k\n","# - trait curvature along the *actual update direction* v (v^T H_trait v), where v is the gradient of L_total\n","# - alignment metrics computed at logging steps:\n","#     * pre:  alignment between g_distill (grad of L_distill_aux) and g_trait\n","#     * post: alignment between g_update  (grad of L_total)      and g_trait\n","# - final performance (student test accuracy on MNIST)\n","#\n","# Key detail:\n","# - The intervention does NOT require the trait gradient to form the update.\n","# - Therefore, audit batches are drawn only at logging steps (same as control).\n","#   At a logging step k, the same audit batches are used to compute:\n","#     (i) trait loss, (ii) trait curvature, and (iii) g_trait for alignment metrics.\n","#\n","# Output:\n","# - **one CSV per (seed, condition)**: runs/.../seed_01/control_metrics.csv, intervention_metrics.csv"],"metadata":{"id":"ucUxp_zBFZG9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["@dataclass\n","class ExperimentConfig:\n","    # Runs\n","    seeds: List[int] = None # e.g., [1,2,...]\n","    out_dir: str = \"./runs_mnist_liminal_anchor\"\n","\n","    # Data\n","    batch_size: int = 1024\n","    num_workers: int = 0  # keep 0 for strict determinism\n","    audit_size: int = 10_000\n","    noise_dataset_size: int = 60_000\n","\n","    # Model (MLP from paper)\n","    hidden_dim: int = 256\n","    aux_m: int = 3\n","\n","    # Training\n","    teacher_epochs: int = 5\n","    student_epochs: int = 5\n","    lr_teacher: float = 3e-4\n","    lr_student: float = 3e-4\n","\n","    # Logging\n","    metrics_every_n_steps: int = 50\n","    # Number of audit batches used at logging steps to compute trait loss/curvature and g_trait.\n","    # For control, it's only used at logging steps.\n","    audit_batches_for_trait: int = 1\n","\n","    # Numerics\n","    eps: float = 1e-12\n","\n","    anchor_mode: str = \"all\"\n","    anchor_T: float = 2.0\n","    lambda0: float = 1.0\n","\n","    anchor_schedule: str = \"original\"\n","    anchor_warmup_epochs: int = 1      # Unused if anchor_schedule = \"original\"\n","    anchor_decay_to_zero: bool = True  # Unused if anchor_schedule = \"original\"\n","\n","cfg = ExperimentConfig(\n","    seeds=list(range(1, 11)),\n","    #seeds=[5],\n","    out_dir=\"./runs_mnist_liminal_anchor_early_then_decay_lambda0_1_all_step_original_all\",\n","    batch_size=1024,\n","    num_workers=0,\n","    audit_size=10_000,\n","    noise_dataset_size=60_000,\n","    hidden_dim=256,\n","    aux_m=3,\n","    teacher_epochs=5,\n","    student_epochs=5,\n","    lr_teacher=3e-4,\n","    lr_student=3e-4,\n","    metrics_every_n_steps=1,\n","    audit_batches_for_trait=10,\n","    eps=1e-12,\n","    anchor_mode=\"all\",\n","    anchor_T=2.0,\n","    lambda0=1.0,\n","    anchor_schedule=\"original\",\n","    anchor_warmup_epochs=1,              # Unused if anchor_schedule = \"original\"\n","    anchor_decay_to_zero=True,           # Unused if anchor_schedule = \"original\"\n",")\n","\n","Path(cfg.out_dir).mkdir(parents=True, exist_ok=True)\n","cfg\n","\n","def set_global_seed(seed: int, deterministic: bool = True) -> None:\n","    \"\"\"Seed python, numpy, and torch. Optionally enable deterministic algorithms.\"\"\"\n","    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","\n","    if deterministic:\n","        torch.backends.cudnn.benchmark = False\n","        torch.backends.cudnn.deterministic = True\n","        try:\n","            torch.use_deterministic_algorithms(True)\n","        except Exception as e:\n","            print(\"Warning: could not enable full deterministic algorithms:\", e)\n","\n","def make_torch_generator(seed: int) -> torch.Generator:\n","    g = torch.Generator()\n","    g.manual_seed(seed)\n","    return g\n","\n","class NoiseImages(Dataset):\n","    \"\"\"Deterministic noise dataset: each index produces a reproducible noise image.\"\"\"\n","    def __init__(self, length: int, seed: int, shape=(1, 28, 28), dist: str = \"normal\"):\n","        self.length = int(length)\n","        self.seed = int(seed)\n","        self.shape = tuple(shape)\n","        self.dist = dist\n","\n","    def __len__(self) -> int:\n","        return self.length\n","\n","    def __getitem__(self, idx: int):\n","        # Per-index deterministic generation.\n","        g = torch.Generator()\n","        g.manual_seed(self.seed * 1_000_000 + int(idx))\n","        if self.dist == \"normal\":\n","            x = torch.randn(self.shape, generator=g)\n","        elif self.dist == \"uniform\":\n","            x = torch.rand(self.shape, generator=g) * 2 - 1\n","        else:\n","            raise ValueError(f\"Unknown dist: {self.dist}\")\n","        # Dummy label (unused)\n","        y = 0\n","        return x, y\n","\n","def get_mnist_datasets(root: str):\n","    transform = T.Compose([T.ToTensor()])\n","    train = torchvision.datasets.MNIST(root=root, train=True, download=True, transform=transform)\n","    test = torchvision.datasets.MNIST(root=root, train=False, download=True, transform=transform)\n","    return train, test\n","\n","def split_train_audit(train_ds, audit_size: int, seed: int):\n","    n = len(train_ds)\n","    audit_size = min(int(audit_size), n)\n","    train_size = n - audit_size\n","    g = make_torch_generator(seed)\n","    train_split, audit_split = random_split(train_ds, [train_size, audit_size], generator=g)\n","    return train_split, audit_split\n","\n","def make_loader(ds, batch_size: int, shuffle: bool, seed: int, num_workers: int = 0):\n","    # Deterministic shuffling via DataLoader generator.\n","    g = make_torch_generator(seed)\n","    return DataLoader(\n","        ds,\n","        batch_size=batch_size,\n","        shuffle=shuffle,\n","        num_workers=num_workers,\n","        pin_memory=torch.cuda.is_available(),\n","        generator=g,\n","        drop_last=False,\n","    )\n","\n","class MLPClassifier(nn.Module):\n","    \"\"\"MLP from the Subliminal Learning MNIST experiment: (784, 256, 256, 10+m) with ReLU.\"\"\"\n","    def __init__(self, hidden_dim: int = 256, aux_m: int = 3):\n","        super().__init__()\n","        self.hidden_dim = hidden_dim\n","        self.aux_m = aux_m\n","        self.fc1 = nn.Linear(28 * 28, hidden_dim)\n","        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n","        self.fc3 = nn.Linear(hidden_dim, 10 + aux_m)\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        x = x.view(x.size(0), -1)\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = self.fc3(x)\n","        return x\n","\n","def build_model_mlp(cfg: ExperimentConfig) -> nn.Module:\n","    \"\"\"Swap model architecture by changing this builder (or passing another builder to the runner).\"\"\"\n","    return MLPClassifier(hidden_dim=cfg.hidden_dim, aux_m=cfg.aux_m)\n","\n","def logits_regular(logits: torch.Tensor) -> torch.Tensor:\n","    return logits[:, :10]\n","\n","def logits_aux(logits: torch.Tensor, aux_m: int) -> torch.Tensor:\n","    return logits[:, 10:10 + aux_m]\n","\n","@torch.no_grad()\n","def accuracy_on_loader(model: nn.Module, loader: DataLoader, device: torch.device) -> float:\n","    model.eval()\n","    correct, total = 0, 0\n","    for x, y in loader:\n","        x, y = x.to(device), y.to(device)\n","        pred = logits_regular(model(x)).argmax(dim=1)\n","        correct += (pred == y).sum().item()\n","        total += y.numel()\n","    return correct / max(total, 1)\n","\n","def get_params(model: nn.Module) -> List[torch.nn.Parameter]:\n","    return [p for p in model.parameters() if p.requires_grad]\n","\n","def flatten_grads_from_params(params: List[torch.nn.Parameter]) -> torch.Tensor:\n","    \"\"\"Flatten gradients already stored in .grad (e.g., after backward()).\"\"\"\n","    flats = []\n","    for p in params:\n","        if p.grad is None:\n","            flats.append(torch.zeros_like(p).view(-1))\n","        else:\n","            flats.append(p.grad.detach().view(-1))\n","    return torch.cat(flats)\n","\n","def split_flat_like_params(params: List[torch.nn.Parameter], flat: torch.Tensor) -> List[torch.Tensor]:\n","    out = []\n","    offset = 0\n","    for p in params:\n","        n = p.numel()\n","        out.append(flat[offset:offset + n].view_as(p))\n","        offset += n\n","    assert offset == flat.numel()\n","    return out\n","\n","def train_teacher(\n","    model: nn.Module,\n","    train_loader: DataLoader,\n","    test_loader: DataLoader,\n","    cfg: ExperimentConfig,\n","    device: torch.device,\n",") -> Dict[str, float]:\n","    \"\"\"Train teacher on MNIST CE using regular logits only.\"\"\"\n","    model = model.to(device)\n","    model.train()\n","    opt = torch.optim.Adam(model.parameters(), lr=cfg.lr_teacher)\n","\n","    for _epoch in range(cfg.teacher_epochs):\n","        for x, y in train_loader:\n","            x, y = x.to(device), y.to(device)\n","            opt.zero_grad(set_to_none=True)\n","            loss = F.cross_entropy(logits_regular(model(x)), y)\n","            loss.backward()\n","            opt.step()\n","\n","    teacher_acc = accuracy_on_loader(model, test_loader, device)\n","    return {\"teacher_test_acc\": float(teacher_acc)}\n","\n","def make_infinite_iterator(loader: DataLoader):\n","    it = iter(loader)\n","    while True:\n","        try:\n","            yield next(it)\n","        except StopIteration:\n","            it = iter(loader)\n","\n","@torch.no_grad()\n","def collect_audit_batches(audit_stream, num_batches: int) -> List[Tuple[torch.Tensor, torch.Tensor]]:\n","    return [next(audit_stream) for _ in range(num_batches)]\n","\n","def trait_loss_on_batches(\n","    model: nn.Module,\n","    batches: List[Tuple[torch.Tensor, torch.Tensor]],\n","    device: torch.device,\n",") -> torch.Tensor:\n","    losses = []\n","    for x, y in batches:\n","        x, y = x.to(device), y.to(device)\n","        losses.append(F.cross_entropy(logits_regular(model(x)), y))\n","    return torch.stack(losses).mean()\n","\n","def compute_trait_grad_flat_from_batches(\n","    model: nn.Module,\n","    params: List[torch.nn.Parameter],\n","    batches: List[Tuple[torch.Tensor, torch.Tensor]],\n","    device: torch.device,\n",") -> Tuple[float, torch.Tensor]:\n","    loss = trait_loss_on_batches(model, batches, device)\n","    grads = torch.autograd.grad(loss, params, retain_graph=False, create_graph=False)\n","    g_flat = torch.cat([g.detach().view(-1) for g in grads])\n","    return float(loss.detach()), g_flat\n","\n","def compute_trait_loss_and_curvature_vHv_from_batches(\n","    model: nn.Module,\n","    params: List[torch.nn.Parameter],\n","    batches: List[Tuple[torch.Tensor, torch.Tensor]],\n","    v_flat: torch.Tensor,\n","    device: torch.device,\n",") -> Tuple[float, float, float]:\n","    \"\"\"\n","    Returns:\n","      trait_loss, v^T H v, (v^T H v) / ||v||^2\n","    \"\"\"\n","    loss = trait_loss_on_batches(model, batches, device)\n","\n","    grads = torch.autograd.grad(loss, params, retain_graph=True, create_graph=True)\n","\n","    v_list = split_flat_like_params(params, v_flat.detach())\n","    gv = torch.zeros((), device=device)\n","    for g, v in zip(grads, v_list):\n","        gv = gv + (g * v).sum()\n","\n","    hvp = torch.autograd.grad(gv, params, retain_graph=False, create_graph=False)\n","    hvp_flat = torch.cat([h.detach().view(-1) for h in hvp])\n","\n","    v_det = v_flat.detach()\n","    vHv = float((v_det @ hvp_flat).detach())\n","    v_norm2 = float((v_det @ v_det).detach())\n","    vHv_norm = vHv / max(v_norm2, 1e-30)\n","\n","    return float(loss.detach()), vHv, vHv_norm\n","\n","def distill_loss_aux_only(student: nn.Module, teacher: nn.Module, x_noise: torch.Tensor, aux_m: int) -> torch.Tensor:\n","    with torch.no_grad():\n","        t_aux = logits_aux(teacher(x_noise), aux_m)\n","        t_prob = F.softmax(t_aux, dim=1)\n","\n","    s_aux = logits_aux(student(x_noise), aux_m)\n","    s_logprob = F.log_softmax(s_aux, dim=1)\n","\n","    kl = torch.nn.KLDivLoss(reduction=\"batchmean\")\n","    return kl(s_logprob, t_prob)\n","\n","# Intervention: liminal-training-style regularization\n","def select_logits_for_anchor(logits: torch.Tensor, mode: str, aux_m: int) -> torch.Tensor:\n","    if mode == \"reg10\":\n","        return logits[:, :10]\n","    if mode == \"aux\":\n","        return logits[:, 10:10 + aux_m]\n","    if mode == \"all\":\n","        return logits\n","    raise ValueError(f\"Unknown anchor_mode={mode}\")\n","\n","def kl_anchor_to_base(\n","    student: nn.Module,\n","    base: nn.Module,\n","    x: torch.Tensor,\n","    cfg: ExperimentConfig,\n",") -> torch.Tensor:\n","    \"\"\"\n","    KL(p_base^T || p_student^T) computed on selected logits (reg10 or all).\n","    Uses KLDivLoss(log p_student, p_base) which equals KL(p_base || p_student).\n","    \"\"\"\n","    kl = torch.nn.KLDivLoss(reduction=\"batchmean\")\n","    T = cfg.anchor_T\n","\n","    with torch.no_grad():\n","        b = select_logits_for_anchor(base(x), cfg.anchor_mode, cfg.aux_m) / T\n","        p_base = F.softmax(b, dim=1)\n","\n","    s = select_logits_for_anchor(student(x), cfg.anchor_mode, cfg.aux_m) / T\n","    logp_student = F.log_softmax(s, dim=1)\n","\n","    return (T ** 2) * kl(logp_student, p_base)\n","\n","def anchor_lambda(epoch: int, global_step: int, steps_per_epoch: int, cfg: ExperimentConfig) -> float:\n","    \"\"\"\n","    Schedule for lambda_k:\n","      - constant: lambda0 throughout\n","      - early_then_decay: lambda0 for first `anchor_warmup_epochs` epochs,\n","        then (optionally) linear decay to 0 by end of training\n","    \"\"\"\n","    if cfg.lambda0 <= 0:\n","        return 0.0\n","\n","    if cfg.anchor_schedule == \"constant\":\n","        return float(cfg.lambda0)\n","\n","    if cfg.anchor_schedule == \"early_then_decay\":\n","        # Warmup phase\n","        if epoch < cfg.anchor_warmup_epochs:\n","            return float(cfg.lambda0)\n","\n","        # After warmup\n","        if not cfg.anchor_decay_to_zero:\n","            return float(cfg.lambda0)\n","\n","        total_steps = steps_per_epoch * cfg.student_epochs\n","        warmup_steps = steps_per_epoch * cfg.anchor_warmup_epochs\n","        remaining = max(total_steps - warmup_steps, 1)\n","\n","        t = (global_step - warmup_steps) / remaining  # This does not perfectly become 0 at the end\n","        return float(cfg.lambda0 * max(0.0, 1.0 - t))\n","\n","    if cfg.anchor_schedule == \"original\":\n","        return float(anchor_lambda_original(global_step, steps_per_epoch, cfg))\n","\n","    raise ValueError(f\"Unknown anchor_schedule={cfg.anchor_schedule}\")\n","\n","def anchor_lambda_original(global_step: int, steps_per_epoch: int, cfg: ExperimentConfig) -> float:\n","    \"\"\"\n","    Implements Yanagisawa et al. (Liminal Training) Eq. (3)-(4):\n","      - Phase 1 (first epoch): anneal from lambda0 to 1 (except lambda0 = 1 case)\n","      - Phase 2 (rest): decay linearly to 0 with scale lambda0\n","    \"\"\"\n","    lambda0 = float(cfg.lambda0)\n","    if lambda0 <= 0:\n","        return 0.0\n","\n","    total_steps = steps_per_epoch * cfg.student_epochs\n","    if total_steps <= 1:\n","        return lambda0\n","\n","    # normalized training progress t in [0,1]\n","    t = global_step / (total_steps - 1)\n","\n","    E = float(cfg.student_epochs)\n","    tau2 = 1.0 / E  # end of first epoch in normalized time\n","\n","    if t <= tau2 + 1e-12:\n","        # Phase 1: lambda(t) = 1 + (lambda0 - 1) * (1 - t/tau2)\n","        s = t / max(tau2, 1e-12)\n","        return 1.0 + (lambda0 - 1.0) * (1.0 - s)\n","    else:\n","        # Phase 2: lambda(t) = lambda0 * (1 - (t - tau2)/(1 - tau2))\n","        s = (t - tau2) / max(1.0 - tau2, 1e-12)\n","        return lambda0 * max(0.0, 1.0 - s)\n","\n","# Student run (control vs intervention)\n","# Key detail (liminal anchoring):\n","# - In **intervention**, the update is computed from\n","#       L_total = L_distill_aux + lambda_k * KL_anchor(student || base)\n","#   so it does NOT require the trait gradient.\n","# - Therefore, audit batches are drawn only at logging steps (same as control),\n","#   and are used solely to compute:\n","#       (i) trait loss / curvature, and\n","#       (ii) alignment metrics between g_trait and the actual update gradient g_update.\n","# - At a logging step k, the same audit batches are used for trait loss, curvature,\n","#   and g_trait (for alignment), ensuring consistency of logged quantities.\n","def run_student_condition(\n","    condition: str,  # \"control\" or \"intervention\"\n","    student: nn.Module,\n","    teacher: nn.Module,\n","    base_model: nn.Module,\n","    noise_loader: DataLoader,\n","    audit_loader: DataLoader,\n","    test_loader: DataLoader,\n","    cfg: ExperimentConfig,\n","    seed: int,\n","    device: torch.device,\n",") -> Tuple[pd.DataFrame, Dict[str, float]]:\n","    assert condition in (\"control\", \"intervention\")\n","    assert cfg.audit_batches_for_trait >= 1\n","\n","    student = student.to(device)\n","    teacher = teacher.to(device)\n","    teacher.eval()\n","    for p in teacher.parameters():\n","        p.requires_grad_(False)\n","\n","    params = get_params(student)\n","    opt = torch.optim.Adam(student.parameters(), lr=cfg.lr_student)\n","\n","    audit_stream = make_infinite_iterator(audit_loader)\n","\n","    logs: List[Dict[str, float]] = []\n","    global_step = 0\n","    steps_per_epoch = len(noise_loader)\n","\n","    student.train()\n","    for epoch in range(cfg.student_epochs):\n","        for x_noise, _ in noise_loader:\n","            x_noise = x_noise.to(device)\n","#\n","            # Decide if we log this step (needed before computing g_distill)\n","            do_log = (global_step % cfg.metrics_every_n_steps == 0)\n","\n","            # --- Forward losses ---\n","            opt.zero_grad(set_to_none=True)\n","\n","            dloss = distill_loss_aux_only(student, teacher, x_noise, cfg.aux_m)\n","\n","            lam = 0.0\n","            anchor_loss = torch.tensor(0.0, device=device)\n","\n","            if condition == \"intervention\":\n","                lam = anchor_lambda(epoch, global_step, steps_per_epoch, cfg)\n","                if lam > 0.0:\n","                    anchor_loss = kl_anchor_to_base(student, base_model, x_noise, cfg)\n","\n","            # If logging, compute distill-only gradient (pre) without touching .grad\n","            g_distill = None\n","            if do_log:\n","                grads_distill = torch.autograd.grad(dloss, params, retain_graph=True, create_graph=False)\n","                g_distill = torch.cat([g.detach().view(-1) for g in grads_distill])\n","\n","            # --- Backward actual update gradient ---\n","            total_loss = dloss + lam * anchor_loss\n","            total_loss.backward()\n","\n","            # Actual update direction (post)\n","            g_update = flatten_grads_from_params(params)\n","#\n","            # --- logging: trait loss + curvature + alignment (pre vs post) ---\n","            if do_log:\n","                # Draw trait batches only at logging steps (both conditions)\n","                trait_batches_k = collect_audit_batches(audit_stream, cfg.audit_batches_for_trait)\n","                # Trait grad on these batches\n","                _trait_loss_tmp, g_trait = compute_trait_grad_flat_from_batches(\n","                    model=student,\n","                    params=params,\n","                    batches=trait_batches_k,\n","                    device=device,\n","                )\n","\n","                # Trait loss + curvature along actual update direction g_update\n","                trait_loss_val, vHv, vHv_norm = compute_trait_loss_and_curvature_vHv_from_batches(\n","                    model=student,\n","                    params=params,\n","                    batches=trait_batches_k,\n","                    v_flat=g_update,\n","                    device=device,\n","                )\n","\n","                # Alignment metrics: pre (distill-only) vs post (actual update)\n","                inner_pre = float((g_distill @ g_trait).detach()) if g_distill is not None else np.nan\n","                cos_pre = float((g_distill @ g_trait).detach() / ((g_distill.norm() * g_trait.norm()).clamp_min(cfg.eps))) if g_distill is not None else np.nan\n","\n","                inner_post = float((g_update @ g_trait).detach())\n","                cos_post = float((g_update @ g_trait).detach() / ((g_update.norm() * g_trait.norm()).clamp_min(cfg.eps)))\n","\n","                logs.append({\n","                    \"seed\": seed,\n","                    \"condition\": condition,\n","                    \"step\": global_step,\n","                    \"epoch\": epoch,\n","\n","                    \"trait_loss\": trait_loss_val,\n","                    \"distill_loss\": float(dloss.detach()),\n","                    \"total_loss\": float(total_loss.detach()),\n","\n","                    \"trait_curvature_vHv\": vHv,\n","                    \"trait_curvature_vHv_norm\": vHv_norm,\n","\n","                    \"inner_pre\": inner_pre,\n","                    \"cos_pre\": cos_pre,\n","                    \"inner_post\": inner_post,\n","                    \"cos_post\": cos_post,\n","\n","                    \"lambda_k\": float(lam),\n","                    \"anchor_loss\": float(anchor_loss.detach()),\n","                    \"anchor_mode\": cfg.anchor_mode,\n","                    \"anchor_schedule\": cfg.anchor_schedule,\n","                })\n","\n","            opt.step()\n","            global_step += 1\n","\n","    student_acc = accuracy_on_loader(student, test_loader, device)\n","\n","    df = pd.DataFrame(logs)\n","    info = {\n","        \"student_test_acc\": float(student_acc),\n","        \"total_steps\": int(global_step),\n","        \"num_logged_rows\": int(len(df)),\n","    }\n","\n","    return df, info\n","\n","# Aggregate summary across runs\n","def aggregate_across_runs(out_dir: str, epoch=None) -> pd.DataFrame:\n","    out_dir = Path(out_dir)\n","    rows = []\n","    for seed_dir in sorted(out_dir.glob(\"seed_*\")):\n","        for cond, fname in [(\"control\", \"control_metrics.csv\"), (\"intervention\", \"intervention_metrics.csv\")]:\n","            p = seed_dir / fname\n","            if not p.exists():\n","                continue\n","\n","            df = pd.read_csv(p)\n","            if len(df) == 0:\n","                continue\n","\n","            if epoch is not None:\n","                if isinstance(epoch, (list, tuple, set, np.ndarray, pd.Series)):\n","                    df = df[df[\"epoch\"].isin(list(epoch))]\n","                else:\n","                    df = df[df[\"epoch\"] == int(epoch)]\n","                if len(df) == 0:\n","                    continue\n","\n","            # Helpers for safe means\n","            def _mean(col: str):\n","                return float(df[col].mean()) if col in df.columns else np.nan\n","\n","            rows.append({\n","                \"seed\": int(df[\"seed\"].iloc[0]),\n","                \"condition\": cond,\n","\n","                # performance\n","                \"teacher_test_acc\": float(df[\"teacher_test_acc\"].iloc[0]) if \"teacher_test_acc\" in df.columns else np.nan,\n","                \"student_test_acc\": float(df[\"student_test_acc\"].iloc[0]) if \"student_test_acc\" in df.columns else np.nan,\n","\n","                # losses\n","                \"mean_trait_loss\": _mean(\"trait_loss\"),\n","                \"mean_distill_loss\": _mean(\"distill_loss\"),\n","\n","                # curvature\n","                \"mean_trait_curvature_vHv\": _mean(\"trait_curvature_vHv\"),\n","                \"mean_trait_curvature_vHv_norm\": _mean(\"trait_curvature_vHv_norm\"),\n","\n","                # alignment summaries\n","                \"mean_inner_pre\": _mean(\"inner_pre\"),\n","                \"mean_cos_pre\": _mean(\"cos_pre\"),\n","                \"mean_inner_post\": _mean(\"inner_post\"),\n","                \"mean_cos_post\": _mean(\"cos_post\"),\n","\n","                # fractions of positive alignment (pre/post)\n","                \"frac_pos_pre\": float((df[\"inner_pre\"] > 0).mean()) if \"inner_pre\" in df.columns else np.nan,\n","                \"frac_pos_post\": float((df[\"inner_post\"] > 0).mean()) if \"inner_post\" in df.columns else np.nan,\n","\n","                \"num_logged_rows\": int(len(df)),\n","            })\n","\n","    return pd.DataFrame(rows).sort_values([\"seed\", \"condition\"]).reset_index(drop=True)\n","\n","# One seed: train teacher once, run control + intervention students\n","def run_one_seed(\n","    seed: int,\n","    cfg: ExperimentConfig,\n","    build_model_fn: Callable[[ExperimentConfig], nn.Module],\n","    device: torch.device,\n",") -> Dict[str, Path]:\n","    set_global_seed(seed, deterministic=True)\n","\n","    run_dir = Path(cfg.out_dir) / f\"seed_{seed:02d}\"\n","    run_dir.mkdir(parents=True, exist_ok=True)\n","\n","    # Data\n","    data_root = str(Path(cfg.out_dir) / \"data_cache\")\n","    mnist_train, mnist_test = get_mnist_datasets(root=data_root)\n","    train_split, audit_split = split_train_audit(mnist_train, audit_size=cfg.audit_size, seed=seed)\n","\n","    train_loader = make_loader(train_split, cfg.batch_size, shuffle=True,  seed=seed + 100, num_workers=cfg.num_workers)\n","    audit_loader = make_loader(audit_split, cfg.batch_size, shuffle=True,  seed=seed + 200, num_workers=cfg.num_workers)\n","    test_loader  = make_loader(mnist_test,  cfg.batch_size, shuffle=False, seed=seed + 300, num_workers=cfg.num_workers)\n","\n","    # Same noise ordering for both conditions\n","    noise_ds = NoiseImages(length=cfg.noise_dataset_size, seed=seed + 400, shape=(1, 28, 28), dist=\"normal\")\n","    noise_loader_control = make_loader(noise_ds, cfg.batch_size, shuffle=True, seed=seed + 500, num_workers=cfg.num_workers)\n","    noise_loader_interv  = make_loader(noise_ds, cfg.batch_size, shuffle=True, seed=seed + 500, num_workers=cfg.num_workers)\n","\n","    # Reference init\n","    reference = build_model_fn(cfg)\n","    ref_state = {k: v.clone().detach().cpu() for k, v in reference.state_dict().items()}\n","    # Base model for anchoring (same init as teacher/student), frozen\n","    base_model = build_model_fn(cfg)\n","    base_model.load_state_dict(ref_state)\n","    base_model = base_model.to(device)\n","    base_model.eval()\n","    for p in base_model.parameters():\n","        p.requires_grad_(False)\n","\n","    # Teacher\n","    teacher = build_model_fn(cfg)\n","    teacher.load_state_dict(ref_state)\n","    teacher_info = train_teacher(teacher, train_loader, test_loader, cfg, device)\n","\n","    # Students (same init)\n","    student_control = build_model_fn(cfg); student_control.load_state_dict(ref_state)\n","    student_interv  = build_model_fn(cfg); student_interv.load_state_dict(ref_state)\n","\n","    # Run conditions\n","    control_df, control_info = run_student_condition(\n","        condition=\"control\",\n","        student=student_control,\n","        teacher=teacher,\n","        base_model=base_model,\n","        noise_loader=noise_loader_control,\n","        audit_loader=audit_loader,\n","        test_loader=test_loader,\n","        cfg=cfg,\n","        seed=seed,\n","        device=device,\n","    )\n","    interv_df, interv_info = run_student_condition(\n","        condition=\"intervention\",\n","        student=student_interv,\n","        teacher=teacher,\n","        base_model=base_model,\n","        noise_loader=noise_loader_interv,\n","        audit_loader=audit_loader,\n","        test_loader=test_loader,\n","        cfg=cfg,\n","        seed=seed,\n","        device=device,\n","    )\n","\n","    # Add final perf to all rows\n","    for df, info in [(control_df, control_info), (interv_df, interv_info)]:\n","        df[\"teacher_test_acc\"] = teacher_info[\"teacher_test_acc\"]\n","        df[\"student_test_acc\"] = info[\"student_test_acc\"]\n","        df[\"total_steps\"] = info[\"total_steps\"]\n","\n","    # Save\n","    control_csv = run_dir / \"control_metrics.csv\"\n","    interv_csv  = run_dir / \"intervention_metrics.csv\"\n","    control_df.to_csv(control_csv, index=False)\n","    interv_df.to_csv(interv_csv, index=False)\n","\n","    meta = {\n","        \"seed\": seed,\n","        \"config\": asdict(cfg),\n","        \"teacher_info\": teacher_info,\n","        \"control_info\": control_info,\n","        \"intervention_info\": interv_info,\n","        \"created_at_utc\": time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime()),\n","        \"device\": str(device),\n","    }\n","    with open(run_dir / \"metadata.json\", \"w\") as f:\n","        json.dump(meta, f, indent=2)\n","\n","    print(\n","        f\"[seed {seed}] teacher_acc={teacher_info['teacher_test_acc']:.4f} | \"\n","        f\"control_acc={control_info['student_test_acc']:.4f} | \"\n","        f\"interv_acc={interv_info['student_test_acc']:.4f} | \"\n","        f\"rows(control/interv)={len(control_df)}/{len(interv_df)}\"\n","    )\n","\n","    return {\"control_csv\": control_csv, \"intervention_csv\": interv_csv}\n","\n","# Run all seeds (1..10)\n","all_paths = []\n","for s in cfg.seeds:\n","    all_paths.append(run_one_seed(seed=s, cfg=cfg, build_model_fn=build_model_mlp, device=device))\n","\n","all_paths[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fHgQZVqknRJY","executionInfo":{"status":"ok","timestamp":1770199045343,"user_tz":-420,"elapsed":7195045,"user":{"displayName":"Chayanon Kitkana","userId":"07549743553674769868"}},"outputId":"b672c444-13f5-4223-eda6-bdb0cb9966a2"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 9.91M/9.91M [00:01<00:00, 6.70MB/s]\n","100%|██████████| 28.9k/28.9k [00:00<00:00, 158kB/s]\n","100%|██████████| 1.65M/1.65M [00:01<00:00, 1.49MB/s]\n","100%|██████████| 4.54k/4.54k [00:00<00:00, 8.24MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["[seed 1] teacher_acc=0.9296 | control_acc=0.1171 | interv_acc=0.1106 | rows(control/interv)=295/295\n","[seed 2] teacher_acc=0.9276 | control_acc=0.1760 | interv_acc=0.1157 | rows(control/interv)=295/295\n","[seed 3] teacher_acc=0.9315 | control_acc=0.3504 | interv_acc=0.2274 | rows(control/interv)=295/295\n","[seed 4] teacher_acc=0.9291 | control_acc=0.1962 | interv_acc=0.2416 | rows(control/interv)=295/295\n","[seed 5] teacher_acc=0.9296 | control_acc=0.4617 | interv_acc=0.2266 | rows(control/interv)=295/295\n","[seed 6] teacher_acc=0.9322 | control_acc=0.3791 | interv_acc=0.2389 | rows(control/interv)=295/295\n","[seed 7] teacher_acc=0.9332 | control_acc=0.3537 | interv_acc=0.1842 | rows(control/interv)=295/295\n","[seed 8] teacher_acc=0.9318 | control_acc=0.2055 | interv_acc=0.1156 | rows(control/interv)=295/295\n","[seed 9] teacher_acc=0.9304 | control_acc=0.3518 | interv_acc=0.2604 | rows(control/interv)=295/295\n","[seed 10] teacher_acc=0.9303 | control_acc=0.1048 | interv_acc=0.1454 | rows(control/interv)=295/295\n"]},{"output_type":"execute_result","data":{"text/plain":["{'control_csv': PosixPath('runs_mnist_liminal_anchor_early_then_decay_lambda0_1_all_step_original_all/seed_01/control_metrics.csv'),\n"," 'intervention_csv': PosixPath('runs_mnist_liminal_anchor_early_then_decay_lambda0_1_all_step_original_all/seed_01/intervention_metrics.csv')}"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["summary = aggregate_across_runs(cfg.out_dir, epoch=0)\n","summary_path = Path(cfg.out_dir) / \"summary_by_seed_condition_epoch0.csv\"\n","summary.to_csv(summary_path, index=False)\n","summary = aggregate_across_runs(cfg.out_dir)\n","summary_path = Path(cfg.out_dir) / \"summary_by_seed_condition_all_epoch.csv\"\n","summary.to_csv(summary_path, index=False)\n","summary"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":696},"id":"_hd8l08Dg898","executionInfo":{"status":"ok","timestamp":1770199045499,"user_tz":-420,"elapsed":154,"user":{"displayName":"Chayanon Kitkana","userId":"07549743553674769868"}},"outputId":"fee3b135-7a7d-4636-d55f-20f54f757c34"},"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["    seed     condition  teacher_test_acc  student_test_acc  mean_trait_loss  \\\n","0      1       control            0.9296            0.1171         2.266227   \n","1      1  intervention            0.9296            0.1106         2.290538   \n","2      2       control            0.9276            0.1760         2.248690   \n","3      2  intervention            0.9276            0.1157         2.288977   \n","4      3       control            0.9315            0.3504         2.261541   \n","5      3  intervention            0.9315            0.2274         2.294408   \n","6      4       control            0.9291            0.1962         2.281892   \n","7      4  intervention            0.9291            0.2416         2.293795   \n","8      5       control            0.9296            0.4617         2.245084   \n","9      5  intervention            0.9296            0.2266         2.287129   \n","10     6       control            0.9322            0.3791         2.242528   \n","11     6  intervention            0.9322            0.2389         2.289672   \n","12     7       control            0.9332            0.3537         2.250006   \n","13     7  intervention            0.9332            0.1842         2.287605   \n","14     8       control            0.9318            0.2055         2.251937   \n","15     8  intervention            0.9318            0.1156         2.290446   \n","16     9       control            0.9304            0.3518         2.253798   \n","17     9  intervention            0.9304            0.2604         2.292401   \n","18    10       control            0.9303            0.1048         2.273142   \n","19    10  intervention            0.9303            0.1454         2.295286   \n","\n","    mean_distill_loss  mean_trait_curvature_vHv  \\\n","0            0.004177              1.358804e-06   \n","1            0.004914              1.255712e-06   \n","2            0.005090              1.632704e-06   \n","3            0.006210              2.204672e-06   \n","4            0.005370              1.291556e-06   \n","5            0.006305              1.388646e-06   \n","6            0.004539              2.827718e-06   \n","7            0.005584              6.372073e-06   \n","8            0.004345              3.776265e-07   \n","9            0.004877              6.911313e-07   \n","10           0.004776              3.485996e-07   \n","11           0.005452              5.858536e-07   \n","12           0.004268              9.039515e-07   \n","13           0.004911              2.215596e-06   \n","14           0.003558              5.419953e-07   \n","15           0.004111              5.149461e-07   \n","16           0.004715              3.906554e-07   \n","17           0.005474              6.712588e-07   \n","18           0.005953              1.204722e-06   \n","19           0.007059              1.313711e-06   \n","\n","    mean_trait_curvature_vHv_norm  mean_inner_pre  mean_cos_pre  \\\n","0                        0.001369        0.000052      0.006719   \n","1                        0.002575       -0.000149     -0.006627   \n","2                        0.001696        0.000055      0.007020   \n","3                        0.003272        0.000218      0.009470   \n","4                        0.001499        0.000052      0.006026   \n","5                        0.002306        0.000059      0.002724   \n","6                        0.001667        0.000034      0.005548   \n","7                        0.007422       -0.000234     -0.008345   \n","8                        0.001307        0.000055      0.006611   \n","9                        0.001875        0.000078      0.005937   \n","10                       0.001124        0.000058      0.007082   \n","11                       0.001783       -0.000052     -0.001878   \n","12                       0.001891        0.000054      0.007763   \n","13                       0.005757        0.000131      0.007659   \n","14                       0.001474        0.000044      0.006979   \n","15                       0.001430        0.000067      0.004750   \n","16                       0.000982        0.000050      0.005819   \n","17                       0.001578        0.000128      0.005348   \n","18                       0.002222        0.000048      0.004164   \n","19                       0.002999       -0.000035     -0.001053   \n","\n","    mean_inner_post  mean_cos_post  frac_pos_pre  frac_pos_post  \\\n","0          0.000052       0.006719      0.722034       0.722034   \n","1          0.000025       0.003333      0.088136       0.674576   \n","2          0.000055       0.007020      0.813559       0.813559   \n","3          0.000009       0.000814      1.000000       0.606780   \n","4          0.000052       0.006026      0.847458       0.847458   \n","5          0.000023       0.003108      0.894915       0.698305   \n","6          0.000034       0.005548      0.698305       0.698305   \n","7          0.000015       0.002173      0.050847       0.576271   \n","8          0.000055       0.006611      0.759322       0.759322   \n","9          0.000022       0.003308      0.996610       0.694915   \n","10         0.000058       0.007082      0.857627       0.857627   \n","11         0.000025       0.003406      0.244068       0.684746   \n","12         0.000054       0.007763      0.928814       0.928814   \n","13         0.000022       0.002782      1.000000       0.684746   \n","14         0.000044       0.006979      0.935593       0.935593   \n","15         0.000016       0.002339      0.993220       0.664407   \n","16         0.000050       0.005819      0.905085       0.905085   \n","17         0.000014       0.001524      0.983051       0.596610   \n","18         0.000048       0.004164      0.725424       0.725424   \n","19         0.000023       0.002512      0.294915       0.596610   \n","\n","    num_logged_rows  \n","0               295  \n","1               295  \n","2               295  \n","3               295  \n","4               295  \n","5               295  \n","6               295  \n","7               295  \n","8               295  \n","9               295  \n","10              295  \n","11              295  \n","12              295  \n","13              295  \n","14              295  \n","15              295  \n","16              295  \n","17              295  \n","18              295  \n","19              295  "],"text/html":["\n","  <div id=\"df-d9f57a3b-1129-4a1e-9f0b-50b541d82765\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>seed</th>\n","      <th>condition</th>\n","      <th>teacher_test_acc</th>\n","      <th>student_test_acc</th>\n","      <th>mean_trait_loss</th>\n","      <th>mean_distill_loss</th>\n","      <th>mean_trait_curvature_vHv</th>\n","      <th>mean_trait_curvature_vHv_norm</th>\n","      <th>mean_inner_pre</th>\n","      <th>mean_cos_pre</th>\n","      <th>mean_inner_post</th>\n","      <th>mean_cos_post</th>\n","      <th>frac_pos_pre</th>\n","      <th>frac_pos_post</th>\n","      <th>num_logged_rows</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>control</td>\n","      <td>0.9296</td>\n","      <td>0.1171</td>\n","      <td>2.266227</td>\n","      <td>0.004177</td>\n","      <td>1.358804e-06</td>\n","      <td>0.001369</td>\n","      <td>0.000052</td>\n","      <td>0.006719</td>\n","      <td>0.000052</td>\n","      <td>0.006719</td>\n","      <td>0.722034</td>\n","      <td>0.722034</td>\n","      <td>295</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>intervention</td>\n","      <td>0.9296</td>\n","      <td>0.1106</td>\n","      <td>2.290538</td>\n","      <td>0.004914</td>\n","      <td>1.255712e-06</td>\n","      <td>0.002575</td>\n","      <td>-0.000149</td>\n","      <td>-0.006627</td>\n","      <td>0.000025</td>\n","      <td>0.003333</td>\n","      <td>0.088136</td>\n","      <td>0.674576</td>\n","      <td>295</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>control</td>\n","      <td>0.9276</td>\n","      <td>0.1760</td>\n","      <td>2.248690</td>\n","      <td>0.005090</td>\n","      <td>1.632704e-06</td>\n","      <td>0.001696</td>\n","      <td>0.000055</td>\n","      <td>0.007020</td>\n","      <td>0.000055</td>\n","      <td>0.007020</td>\n","      <td>0.813559</td>\n","      <td>0.813559</td>\n","      <td>295</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2</td>\n","      <td>intervention</td>\n","      <td>0.9276</td>\n","      <td>0.1157</td>\n","      <td>2.288977</td>\n","      <td>0.006210</td>\n","      <td>2.204672e-06</td>\n","      <td>0.003272</td>\n","      <td>0.000218</td>\n","      <td>0.009470</td>\n","      <td>0.000009</td>\n","      <td>0.000814</td>\n","      <td>1.000000</td>\n","      <td>0.606780</td>\n","      <td>295</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>3</td>\n","      <td>control</td>\n","      <td>0.9315</td>\n","      <td>0.3504</td>\n","      <td>2.261541</td>\n","      <td>0.005370</td>\n","      <td>1.291556e-06</td>\n","      <td>0.001499</td>\n","      <td>0.000052</td>\n","      <td>0.006026</td>\n","      <td>0.000052</td>\n","      <td>0.006026</td>\n","      <td>0.847458</td>\n","      <td>0.847458</td>\n","      <td>295</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>3</td>\n","      <td>intervention</td>\n","      <td>0.9315</td>\n","      <td>0.2274</td>\n","      <td>2.294408</td>\n","      <td>0.006305</td>\n","      <td>1.388646e-06</td>\n","      <td>0.002306</td>\n","      <td>0.000059</td>\n","      <td>0.002724</td>\n","      <td>0.000023</td>\n","      <td>0.003108</td>\n","      <td>0.894915</td>\n","      <td>0.698305</td>\n","      <td>295</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>4</td>\n","      <td>control</td>\n","      <td>0.9291</td>\n","      <td>0.1962</td>\n","      <td>2.281892</td>\n","      <td>0.004539</td>\n","      <td>2.827718e-06</td>\n","      <td>0.001667</td>\n","      <td>0.000034</td>\n","      <td>0.005548</td>\n","      <td>0.000034</td>\n","      <td>0.005548</td>\n","      <td>0.698305</td>\n","      <td>0.698305</td>\n","      <td>295</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>4</td>\n","      <td>intervention</td>\n","      <td>0.9291</td>\n","      <td>0.2416</td>\n","      <td>2.293795</td>\n","      <td>0.005584</td>\n","      <td>6.372073e-06</td>\n","      <td>0.007422</td>\n","      <td>-0.000234</td>\n","      <td>-0.008345</td>\n","      <td>0.000015</td>\n","      <td>0.002173</td>\n","      <td>0.050847</td>\n","      <td>0.576271</td>\n","      <td>295</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>5</td>\n","      <td>control</td>\n","      <td>0.9296</td>\n","      <td>0.4617</td>\n","      <td>2.245084</td>\n","      <td>0.004345</td>\n","      <td>3.776265e-07</td>\n","      <td>0.001307</td>\n","      <td>0.000055</td>\n","      <td>0.006611</td>\n","      <td>0.000055</td>\n","      <td>0.006611</td>\n","      <td>0.759322</td>\n","      <td>0.759322</td>\n","      <td>295</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>5</td>\n","      <td>intervention</td>\n","      <td>0.9296</td>\n","      <td>0.2266</td>\n","      <td>2.287129</td>\n","      <td>0.004877</td>\n","      <td>6.911313e-07</td>\n","      <td>0.001875</td>\n","      <td>0.000078</td>\n","      <td>0.005937</td>\n","      <td>0.000022</td>\n","      <td>0.003308</td>\n","      <td>0.996610</td>\n","      <td>0.694915</td>\n","      <td>295</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>6</td>\n","      <td>control</td>\n","      <td>0.9322</td>\n","      <td>0.3791</td>\n","      <td>2.242528</td>\n","      <td>0.004776</td>\n","      <td>3.485996e-07</td>\n","      <td>0.001124</td>\n","      <td>0.000058</td>\n","      <td>0.007082</td>\n","      <td>0.000058</td>\n","      <td>0.007082</td>\n","      <td>0.857627</td>\n","      <td>0.857627</td>\n","      <td>295</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>6</td>\n","      <td>intervention</td>\n","      <td>0.9322</td>\n","      <td>0.2389</td>\n","      <td>2.289672</td>\n","      <td>0.005452</td>\n","      <td>5.858536e-07</td>\n","      <td>0.001783</td>\n","      <td>-0.000052</td>\n","      <td>-0.001878</td>\n","      <td>0.000025</td>\n","      <td>0.003406</td>\n","      <td>0.244068</td>\n","      <td>0.684746</td>\n","      <td>295</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>7</td>\n","      <td>control</td>\n","      <td>0.9332</td>\n","      <td>0.3537</td>\n","      <td>2.250006</td>\n","      <td>0.004268</td>\n","      <td>9.039515e-07</td>\n","      <td>0.001891</td>\n","      <td>0.000054</td>\n","      <td>0.007763</td>\n","      <td>0.000054</td>\n","      <td>0.007763</td>\n","      <td>0.928814</td>\n","      <td>0.928814</td>\n","      <td>295</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>7</td>\n","      <td>intervention</td>\n","      <td>0.9332</td>\n","      <td>0.1842</td>\n","      <td>2.287605</td>\n","      <td>0.004911</td>\n","      <td>2.215596e-06</td>\n","      <td>0.005757</td>\n","      <td>0.000131</td>\n","      <td>0.007659</td>\n","      <td>0.000022</td>\n","      <td>0.002782</td>\n","      <td>1.000000</td>\n","      <td>0.684746</td>\n","      <td>295</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>8</td>\n","      <td>control</td>\n","      <td>0.9318</td>\n","      <td>0.2055</td>\n","      <td>2.251937</td>\n","      <td>0.003558</td>\n","      <td>5.419953e-07</td>\n","      <td>0.001474</td>\n","      <td>0.000044</td>\n","      <td>0.006979</td>\n","      <td>0.000044</td>\n","      <td>0.006979</td>\n","      <td>0.935593</td>\n","      <td>0.935593</td>\n","      <td>295</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>8</td>\n","      <td>intervention</td>\n","      <td>0.9318</td>\n","      <td>0.1156</td>\n","      <td>2.290446</td>\n","      <td>0.004111</td>\n","      <td>5.149461e-07</td>\n","      <td>0.001430</td>\n","      <td>0.000067</td>\n","      <td>0.004750</td>\n","      <td>0.000016</td>\n","      <td>0.002339</td>\n","      <td>0.993220</td>\n","      <td>0.664407</td>\n","      <td>295</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>9</td>\n","      <td>control</td>\n","      <td>0.9304</td>\n","      <td>0.3518</td>\n","      <td>2.253798</td>\n","      <td>0.004715</td>\n","      <td>3.906554e-07</td>\n","      <td>0.000982</td>\n","      <td>0.000050</td>\n","      <td>0.005819</td>\n","      <td>0.000050</td>\n","      <td>0.005819</td>\n","      <td>0.905085</td>\n","      <td>0.905085</td>\n","      <td>295</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>9</td>\n","      <td>intervention</td>\n","      <td>0.9304</td>\n","      <td>0.2604</td>\n","      <td>2.292401</td>\n","      <td>0.005474</td>\n","      <td>6.712588e-07</td>\n","      <td>0.001578</td>\n","      <td>0.000128</td>\n","      <td>0.005348</td>\n","      <td>0.000014</td>\n","      <td>0.001524</td>\n","      <td>0.983051</td>\n","      <td>0.596610</td>\n","      <td>295</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>10</td>\n","      <td>control</td>\n","      <td>0.9303</td>\n","      <td>0.1048</td>\n","      <td>2.273142</td>\n","      <td>0.005953</td>\n","      <td>1.204722e-06</td>\n","      <td>0.002222</td>\n","      <td>0.000048</td>\n","      <td>0.004164</td>\n","      <td>0.000048</td>\n","      <td>0.004164</td>\n","      <td>0.725424</td>\n","      <td>0.725424</td>\n","      <td>295</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>10</td>\n","      <td>intervention</td>\n","      <td>0.9303</td>\n","      <td>0.1454</td>\n","      <td>2.295286</td>\n","      <td>0.007059</td>\n","      <td>1.313711e-06</td>\n","      <td>0.002999</td>\n","      <td>-0.000035</td>\n","      <td>-0.001053</td>\n","      <td>0.000023</td>\n","      <td>0.002512</td>\n","      <td>0.294915</td>\n","      <td>0.596610</td>\n","      <td>295</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d9f57a3b-1129-4a1e-9f0b-50b541d82765')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-d9f57a3b-1129-4a1e-9f0b-50b541d82765 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-d9f57a3b-1129-4a1e-9f0b-50b541d82765');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","  <div id=\"id_627a7b4a-278c-46a9-aec8-a53a421e4037\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('summary')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_627a7b4a-278c-46a9-aec8-a53a421e4037 button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('summary');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"summary","summary":"{\n  \"name\": \"summary\",\n  \"rows\": 20,\n  \"fields\": [\n    {\n      \"column\": \"seed\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 1,\n        \"max\": 10,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          9,\n          2,\n          6\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"condition\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"intervention\",\n          \"control\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"teacher_test_acc\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0016189990409931888,\n        \"min\": 0.9276,\n        \"max\": 0.9332,\n        \"num_unique_values\": 9,\n        \"samples\": [\n          0.9304,\n          0.9276\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"student_test_acc\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.10371484832538633,\n        \"min\": 0.1048,\n        \"max\": 0.4617,\n        \"num_unique_values\": 20,\n        \"samples\": [\n          0.1171,\n          0.2604\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mean_trait_loss\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.019448451429033068,\n        \"min\": 2.242527513180749,\n        \"max\": 2.2952855126332428,\n        \"num_unique_values\": 20,\n        \"samples\": [\n          2.266226873559467,\n          2.2924005767046394\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mean_distill_loss\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0008576757924951273,\n        \"min\": 0.0035576195400839144,\n        \"max\": 0.007059228368166597,\n        \"num_unique_values\": 20,\n        \"samples\": [\n          0.004177246420738887,\n          0.005473694197391507\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mean_trait_curvature_vHv\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.3540915129202928e-06,\n        \"min\": 3.4859956443963656e-07,\n        \"max\": 6.3720730167009315e-06,\n        \"num_unique_values\": 20,\n        \"samples\": [\n          1.3588043028082386e-06,\n          6.712588443901006e-07\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mean_trait_curvature_vHv_norm\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0016002845861199737,\n        \"min\": 0.0009817723894930284,\n        \"max\": 0.007421988241069183,\n        \"num_unique_values\": 20,\n        \"samples\": [\n          0.0013689607135911666,\n          0.0015778671620138459\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mean_inner_pre\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 9.63018604392852e-05,\n        \"min\": -0.0002340440945124756,\n        \"max\": 0.0002181797699079464,\n        \"num_unique_values\": 20,\n        \"samples\": [\n          5.21921255482417e-05,\n          0.00012759320368597933\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mean_cos_pre\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.004823433474276809,\n        \"min\": -0.008345392180576585,\n        \"max\": 0.009470478170630235,\n        \"num_unique_values\": 20,\n        \"samples\": [\n          0.006719254345414605,\n          0.005348349172006299\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mean_inner_post\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.697142234015333e-05,\n        \"min\": 8.850711179486795e-06,\n        \"max\": 5.8306132092885086e-05,\n        \"num_unique_values\": 20,\n        \"samples\": [\n          5.21921255482417e-05,\n          1.411295383519918e-05\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mean_cos_post\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0021736943975915494,\n        \"min\": 0.0008144676940484973,\n        \"max\": 0.007762861997878726,\n        \"num_unique_values\": 20,\n        \"samples\": [\n          0.006719254345414605,\n          0.0015236381250761579\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"frac_pos_pre\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.31005642782153503,\n        \"min\": 0.05084745762711865,\n        \"max\": 1.0,\n        \"num_unique_values\": 19,\n        \"samples\": [\n          0.7220338983050848,\n          0.8949152542372881\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"frac_pos_post\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.11223090289036164,\n        \"min\": 0.576271186440678,\n        \"max\": 0.9355932203389831,\n        \"num_unique_values\": 17,\n        \"samples\": [\n          0.7220338983050848,\n          0.6745762711864407\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"num_logged_rows\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 295,\n        \"max\": 295,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          295\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["!zip -r runs_mnist_liminal_anchor_early_then_decay_lambda0_1_all_step_original_all.zip ./runs_mnist_liminal_anchor_early_then_decay_lambda0_1_all_step_original_all"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wYHnq8HBjFOT","executionInfo":{"status":"ok","timestamp":1770199047290,"user_tz":-420,"elapsed":1790,"user":{"displayName":"Chayanon Kitkana","userId":"07549743553674769868"}},"outputId":"9f33d36c-29ca-45de-b6ae-ed50c922b273"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["  adding: runs_mnist_liminal_anchor_early_then_decay_lambda0_1_all_step_original_all/ (stored 0%)\n","  adding: runs_mnist_liminal_anchor_early_then_decay_lambda0_1_all_step_original_all/data_cache/ (stored 0%)\n","  adding: runs_mnist_liminal_anchor_early_then_decay_lambda0_1_all_step_original_all/data_cache/MNIST/ (stored 0%)\n","  adding: runs_mnist_liminal_anchor_early_then_decay_lambda0_1_all_step_original_all/data_cache/MNIST/raw/ (stored 0%)\n","  adding: runs_mnist_liminal_anchor_early_then_decay_lambda0_1_all_step_original_all/data_cache/MNIST/raw/t10k-labels-idx1-ubyte (deflated 55%)\n","  adding: runs_mnist_liminal_anchor_early_then_decay_lambda0_1_all_step_original_all/data_cache/MNIST/raw/t10k-images-idx3-ubyte (deflated 79%)\n","  adding: runs_mnist_liminal_anchor_early_then_decay_lambda0_1_all_step_original_all/data_cache/MNIST/raw/t10k-images-idx3-ubyte.gz (deflated 0%)\n","  adding: runs_mnist_liminal_anchor_early_then_decay_lambda0_1_all_step_original_all/data_cache/MNIST/raw/t10k-labels-idx1-ubyte.gz (stored 0%)\n","  adding: runs_mnist_liminal_anchor_early_then_decay_lambda0_1_all_step_original_all/data_cache/MNIST/raw/train-labels-idx1-ubyte (deflated 52%)\n","  adding: runs_mnist_liminal_anchor_early_then_decay_lambda0_1_all_step_original_all/data_cache/MNIST/raw/train-images-idx3-ubyte.gz (deflated 0%)\n","  adding: runs_mnist_liminal_anchor_early_then_decay_lambda0_1_all_step_original_all/data_cache/MNIST/raw/train-images-idx3-ubyte (deflated 79%)\n","  adding: runs_mnist_liminal_anchor_early_then_decay_lambda0_1_all_step_original_all/data_cache/MNIST/raw/train-labels-idx1-ubyte.gz (stored 0%)\n","  adding: runs_mnist_liminal_anchor_early_then_decay_lambda0_1_all_step_original_all/seed_08/ (stored 0%)\n","  adding: runs_mnist_liminal_anchor_early_then_decay_lambda0_1_all_step_original_all/seed_08/control_metrics.csv (deflated 72%)\n","  adding: runs_mnist_liminal_anchor_early_then_decay_lambda0_1_all_step_original_all/seed_08/metadata.json (deflated 58%)\n","  adding: runs_mnist_liminal_anchor_early_then_decay_lambda0_1_all_step_original_all/seed_08/intervention_metrics.csv (deflated 62%)\n","  adding: runs_mnist_liminal_anchor_early_then_decay_lambda0_1_all_step_original_all/seed_03/ (stored 0%)\n","  adding: runs_mnist_liminal_anchor_early_then_decay_lambda0_1_all_step_original_all/seed_03/control_metrics.csv (deflated 72%)\n","  adding: runs_mnist_liminal_anchor_early_then_decay_lambda0_1_all_step_original_all/seed_03/metadata.json (deflated 58%)\n","  adding: runs_mnist_liminal_anchor_early_then_decay_lambda0_1_all_step_original_all/seed_03/intervention_metrics.csv (deflated 62%)\n","  adding: runs_mnist_liminal_anchor_early_then_decay_lambda0_1_all_step_original_all/seed_05/ (stored 0%)\n","  adding: runs_mnist_liminal_anchor_early_then_decay_lambda0_1_all_step_original_all/seed_05/control_metrics.csv (deflated 72%)\n","  adding: runs_mnist_liminal_anchor_early_then_decay_lambda0_1_all_step_original_all/seed_05/metadata.json (deflated 58%)\n","  adding: runs_mnist_liminal_anchor_early_then_decay_lambda0_1_all_step_original_all/seed_05/intervention_metrics.csv (deflated 62%)\n","  adding: runs_mnist_liminal_anchor_early_then_decay_lambda0_1_all_step_original_all/seed_06/ (stored 0%)\n","  adding: runs_mnist_liminal_anchor_early_then_decay_lambda0_1_all_step_original_all/seed_06/control_metrics.csv (deflated 72%)\n","  adding: runs_mnist_liminal_anchor_early_then_decay_lambda0_1_all_step_original_all/seed_06/metadata.json (deflated 58%)\n","  adding: runs_mnist_liminal_anchor_early_then_decay_lambda0_1_all_step_original_all/seed_06/intervention_metrics.csv (deflated 62%)\n","  adding: runs_mnist_liminal_anchor_early_then_decay_lambda0_1_all_step_original_all/seed_02/ (stored 0%)\n","  adding: runs_mnist_liminal_anchor_early_then_decay_lambda0_1_all_step_original_all/seed_02/control_metrics.csv (deflated 72%)\n","  adding: runs_mnist_liminal_anchor_early_then_decay_lambda0_1_all_step_original_all/seed_02/metadata.json (deflated 58%)\n","  adding: runs_mnist_liminal_anchor_early_then_decay_lambda0_1_all_step_original_all/seed_02/intervention_metrics.csv (deflated 62%)\n","  adding: runs_mnist_liminal_anchor_early_then_decay_lambda0_1_all_step_original_all/seed_10/ (stored 0%)\n","  adding: runs_mnist_liminal_anchor_early_then_decay_lambda0_1_all_step_original_all/seed_10/control_metrics.csv (deflated 72%)\n","  adding: runs_mnist_liminal_anchor_early_then_decay_lambda0_1_all_step_original_all/seed_10/metadata.json (deflated 58%)\n","  adding: runs_mnist_liminal_anchor_early_then_decay_lambda0_1_all_step_original_all/seed_10/intervention_metrics.csv (deflated 62%)\n","  adding: runs_mnist_liminal_anchor_early_then_decay_lambda0_1_all_step_original_all/seed_04/ (stored 0%)\n","  adding: runs_mnist_liminal_anchor_early_then_decay_lambda0_1_all_step_original_all/seed_04/control_metrics.csv (deflated 72%)\n","  adding: runs_mnist_liminal_anchor_early_then_decay_lambda0_1_all_step_original_all/seed_04/metadata.json (deflated 58%)\n","  adding: runs_mnist_liminal_anchor_early_then_decay_lambda0_1_all_step_original_all/seed_04/intervention_metrics.csv (deflated 62%)\n","  adding: runs_mnist_liminal_anchor_early_then_decay_lambda0_1_all_step_original_all/summary_by_seed_condition_all_epoch.csv (deflated 59%)\n","  adding: runs_mnist_liminal_anchor_early_then_decay_lambda0_1_all_step_original_all/seed_07/ (stored 0%)\n","  adding: runs_mnist_liminal_anchor_early_then_decay_lambda0_1_all_step_original_all/seed_07/control_metrics.csv (deflated 72%)\n","  adding: runs_mnist_liminal_anchor_early_then_decay_lambda0_1_all_step_original_all/seed_07/metadata.json (deflated 58%)\n","  adding: runs_mnist_liminal_anchor_early_then_decay_lambda0_1_all_step_original_all/seed_07/intervention_metrics.csv (deflated 62%)\n","  adding: runs_mnist_liminal_anchor_early_then_decay_lambda0_1_all_step_original_all/seed_09/ (stored 0%)\n","  adding: runs_mnist_liminal_anchor_early_then_decay_lambda0_1_all_step_original_all/seed_09/control_metrics.csv (deflated 72%)\n","  adding: runs_mnist_liminal_anchor_early_then_decay_lambda0_1_all_step_original_all/seed_09/metadata.json (deflated 58%)\n","  adding: runs_mnist_liminal_anchor_early_then_decay_lambda0_1_all_step_original_all/seed_09/intervention_metrics.csv (deflated 62%)\n","  adding: runs_mnist_liminal_anchor_early_then_decay_lambda0_1_all_step_original_all/seed_01/ (stored 0%)\n","  adding: runs_mnist_liminal_anchor_early_then_decay_lambda0_1_all_step_original_all/seed_01/control_metrics.csv (deflated 72%)\n","  adding: runs_mnist_liminal_anchor_early_then_decay_lambda0_1_all_step_original_all/seed_01/metadata.json (deflated 58%)\n","  adding: runs_mnist_liminal_anchor_early_then_decay_lambda0_1_all_step_original_all/seed_01/intervention_metrics.csv (deflated 62%)\n","  adding: runs_mnist_liminal_anchor_early_then_decay_lambda0_1_all_step_original_all/summary_by_seed_condition_epoch0.csv (deflated 59%)\n"]}]},{"cell_type":"code","source":["summary_path = \"/content/runs_mnist_liminal_anchor_early_then_decay_lambda0_1_all_step_original_all/summary_by_seed_condition_epoch0.csv\"\n","df = pd.read_csv(summary_path)\n","num_cols = [c for c in df.select_dtypes(include=\"number\").columns if c != \"seed\"]\n","avg_by_condition = (\n","    df.groupby(\"condition\")[num_cols]\n","      .agg([\"mean\", \"std\", \"count\"])\n",")"],"metadata":{"id":"sb_Ab71eeiw4","executionInfo":{"status":"ok","timestamp":1770209690397,"user_tz":-420,"elapsed":42,"user":{"displayName":"Chayanon Kitkana","userId":"07549743553674769868"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["cols = [\"mean_distill_loss\", \"mean_trait_loss\", \"mean_cos_pre\", \"mean_cos_post\", \"student_test_acc\", \"frac_pos_pre\", \"frac_pos_post\"]\n","df.groupby(\"condition\")[cols].agg([\"mean\",\"std\",\"count\"])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":224},"id":"Xq-b7brzejZK","executionInfo":{"status":"ok","timestamp":1770209692540,"user_tz":-420,"elapsed":41,"user":{"displayName":"Chayanon Kitkana","userId":"07549743553674769868"}},"outputId":"b8d15254-bf4b-4f83-eb61-592b2f79b1fd"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["             mean_distill_loss                 mean_trait_loss            \\\n","                          mean       std count            mean       std   \n","condition                                                                  \n","control               0.012524  0.002329    10        2.293811  0.005385   \n","intervention          0.013327  0.002461    10        2.299757  0.003338   \n","\n","                   mean_cos_pre                 mean_cos_post  ...        \\\n","             count         mean       std count          mean  ... count   \n","condition                                                      ...         \n","control         10     0.007518  0.003805    10      0.007518  ...    10   \n","intervention    10     0.004312  0.007086    10      0.002375  ...    10   \n","\n","             student_test_acc                 frac_pos_pre                  \\\n","                         mean       std count         mean       std count   \n","condition                                                                    \n","control               0.26963  0.123794    10     0.808475  0.153283    10   \n","intervention          0.18664  0.059646    10     0.735593  0.353837    10   \n","\n","             frac_pos_post                  \n","                      mean       std count  \n","condition                                   \n","control           0.808475  0.153283    10  \n","intervention      0.628814  0.106134    10  \n","\n","[2 rows x 21 columns]"],"text/html":["\n","  <div id=\"df-ce9c0003-0fba-429e-80e7-16af2c8b5eac\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead tr th {\n","        text-align: left;\n","    }\n","\n","    .dataframe thead tr:last-of-type th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr>\n","      <th></th>\n","      <th colspan=\"3\" halign=\"left\">mean_distill_loss</th>\n","      <th colspan=\"3\" halign=\"left\">mean_trait_loss</th>\n","      <th colspan=\"3\" halign=\"left\">mean_cos_pre</th>\n","      <th colspan=\"3\" halign=\"left\">mean_cos_post</th>\n","      <th colspan=\"3\" halign=\"left\">student_test_acc</th>\n","      <th colspan=\"3\" halign=\"left\">frac_pos_pre</th>\n","      <th colspan=\"3\" halign=\"left\">frac_pos_post</th>\n","    </tr>\n","    <tr>\n","      <th></th>\n","      <th>mean</th>\n","      <th>std</th>\n","      <th>count</th>\n","      <th>mean</th>\n","      <th>std</th>\n","      <th>count</th>\n","      <th>mean</th>\n","      <th>std</th>\n","      <th>count</th>\n","      <th>mean</th>\n","      <th>...</th>\n","      <th>count</th>\n","      <th>mean</th>\n","      <th>std</th>\n","      <th>count</th>\n","      <th>mean</th>\n","      <th>std</th>\n","      <th>count</th>\n","      <th>mean</th>\n","      <th>std</th>\n","      <th>count</th>\n","    </tr>\n","    <tr>\n","      <th>condition</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>control</th>\n","      <td>0.012524</td>\n","      <td>0.002329</td>\n","      <td>10</td>\n","      <td>2.293811</td>\n","      <td>0.005385</td>\n","      <td>10</td>\n","      <td>0.007518</td>\n","      <td>0.003805</td>\n","      <td>10</td>\n","      <td>0.007518</td>\n","      <td>...</td>\n","      <td>10</td>\n","      <td>0.26963</td>\n","      <td>0.123794</td>\n","      <td>10</td>\n","      <td>0.808475</td>\n","      <td>0.153283</td>\n","      <td>10</td>\n","      <td>0.808475</td>\n","      <td>0.153283</td>\n","      <td>10</td>\n","    </tr>\n","    <tr>\n","      <th>intervention</th>\n","      <td>0.013327</td>\n","      <td>0.002461</td>\n","      <td>10</td>\n","      <td>2.299757</td>\n","      <td>0.003338</td>\n","      <td>10</td>\n","      <td>0.004312</td>\n","      <td>0.007086</td>\n","      <td>10</td>\n","      <td>0.002375</td>\n","      <td>...</td>\n","      <td>10</td>\n","      <td>0.18664</td>\n","      <td>0.059646</td>\n","      <td>10</td>\n","      <td>0.735593</td>\n","      <td>0.353837</td>\n","      <td>10</td>\n","      <td>0.628814</td>\n","      <td>0.106134</td>\n","      <td>10</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2 rows × 21 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ce9c0003-0fba-429e-80e7-16af2c8b5eac')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-ce9c0003-0fba-429e-80e7-16af2c8b5eac button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-ce9c0003-0fba-429e-80e7-16af2c8b5eac');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe"}},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["summary_path = \"/content/runs_mnist_liminal_anchor_early_then_decay_lambda0_1_all_step_original_all/summary_by_seed_condition_all_epoch.csv\"\n","df = pd.read_csv(summary_path)\n","num_cols = [c for c in df.select_dtypes(include=\"number\").columns if c != \"seed\"]\n","avg_by_condition = (\n","    df.groupby(\"condition\")[num_cols]\n","      .agg([\"mean\", \"std\", \"count\"])\n",")"],"metadata":{"id":"pxM4XCJkuI_Q","executionInfo":{"status":"ok","timestamp":1770209730887,"user_tz":-420,"elapsed":45,"user":{"displayName":"Chayanon Kitkana","userId":"07549743553674769868"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["cols = [\"mean_distill_loss\", \"mean_trait_loss\", \"mean_cos_pre\", \"mean_cos_post\", \"student_test_acc\", \"frac_pos_pre\", \"frac_pos_post\"]\n","df.groupby(\"condition\")[cols].agg([\"mean\",\"std\",\"count\"])"],"metadata":{"id":"wTGUsZZvuJch","executionInfo":{"status":"ok","timestamp":1770209732736,"user_tz":-420,"elapsed":25,"user":{"displayName":"Chayanon Kitkana","userId":"07549743553674769868"}},"outputId":"6d9280f3-d8f8-48b6-e243-a8192ed5b476","colab":{"base_uri":"https://localhost:8080/","height":224}},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["             mean_distill_loss                 mean_trait_loss            \\\n","                          mean       std count            mean       std   \n","condition                                                                  \n","control               0.004679  0.000674    10        2.257484  0.012862   \n","intervention          0.005490  0.000856    10        2.291026  0.002841   \n","\n","                   mean_cos_pre                 mean_cos_post  ...        \\\n","             count         mean       std count          mean  ... count   \n","condition                                                      ...         \n","control         10     0.006373  0.001021    10      0.006373  ...    10   \n","intervention    10     0.001799  0.006037    10      0.002530  ...    10   \n","\n","             student_test_acc                 frac_pos_pre                  \\\n","                         mean       std count         mean       std count   \n","condition                                                                    \n","control               0.26963  0.123794    10     0.819322  0.089245    10   \n","intervention          0.18664  0.059646    10     0.654576  0.424156    10   \n","\n","             frac_pos_post                  \n","                      mean       std count  \n","condition                                   \n","control           0.819322  0.089245    10  \n","intervention      0.647797  0.047765    10  \n","\n","[2 rows x 21 columns]"],"text/html":["\n","  <div id=\"df-16b18118-db22-4f83-a904-769d61cb8e7d\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead tr th {\n","        text-align: left;\n","    }\n","\n","    .dataframe thead tr:last-of-type th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr>\n","      <th></th>\n","      <th colspan=\"3\" halign=\"left\">mean_distill_loss</th>\n","      <th colspan=\"3\" halign=\"left\">mean_trait_loss</th>\n","      <th colspan=\"3\" halign=\"left\">mean_cos_pre</th>\n","      <th colspan=\"3\" halign=\"left\">mean_cos_post</th>\n","      <th colspan=\"3\" halign=\"left\">student_test_acc</th>\n","      <th colspan=\"3\" halign=\"left\">frac_pos_pre</th>\n","      <th colspan=\"3\" halign=\"left\">frac_pos_post</th>\n","    </tr>\n","    <tr>\n","      <th></th>\n","      <th>mean</th>\n","      <th>std</th>\n","      <th>count</th>\n","      <th>mean</th>\n","      <th>std</th>\n","      <th>count</th>\n","      <th>mean</th>\n","      <th>std</th>\n","      <th>count</th>\n","      <th>mean</th>\n","      <th>...</th>\n","      <th>count</th>\n","      <th>mean</th>\n","      <th>std</th>\n","      <th>count</th>\n","      <th>mean</th>\n","      <th>std</th>\n","      <th>count</th>\n","      <th>mean</th>\n","      <th>std</th>\n","      <th>count</th>\n","    </tr>\n","    <tr>\n","      <th>condition</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>control</th>\n","      <td>0.004679</td>\n","      <td>0.000674</td>\n","      <td>10</td>\n","      <td>2.257484</td>\n","      <td>0.012862</td>\n","      <td>10</td>\n","      <td>0.006373</td>\n","      <td>0.001021</td>\n","      <td>10</td>\n","      <td>0.006373</td>\n","      <td>...</td>\n","      <td>10</td>\n","      <td>0.26963</td>\n","      <td>0.123794</td>\n","      <td>10</td>\n","      <td>0.819322</td>\n","      <td>0.089245</td>\n","      <td>10</td>\n","      <td>0.819322</td>\n","      <td>0.089245</td>\n","      <td>10</td>\n","    </tr>\n","    <tr>\n","      <th>intervention</th>\n","      <td>0.005490</td>\n","      <td>0.000856</td>\n","      <td>10</td>\n","      <td>2.291026</td>\n","      <td>0.002841</td>\n","      <td>10</td>\n","      <td>0.001799</td>\n","      <td>0.006037</td>\n","      <td>10</td>\n","      <td>0.002530</td>\n","      <td>...</td>\n","      <td>10</td>\n","      <td>0.18664</td>\n","      <td>0.059646</td>\n","      <td>10</td>\n","      <td>0.654576</td>\n","      <td>0.424156</td>\n","      <td>10</td>\n","      <td>0.647797</td>\n","      <td>0.047765</td>\n","      <td>10</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2 rows × 21 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-16b18118-db22-4f83-a904-769d61cb8e7d')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-16b18118-db22-4f83-a904-769d61cb8e7d button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-16b18118-db22-4f83-a904-769d61cb8e7d');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe"}},"metadata":{},"execution_count":8}]}]}