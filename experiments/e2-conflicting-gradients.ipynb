{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN7wvxEPDuV1FdN65z6eZmC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VPr_IBh8m29p","executionInfo":{"status":"ok","timestamp":1768765165469,"user_tz":-420,"elapsed":13808,"user":{"displayName":"Chayanon Kitkana","userId":"07549743553674769868"}},"outputId":"356d0534-aea2-455d-b61b-3724f3aede7f"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch: 2.9.0+cpu\n","torchvision: 0.24.0+cpu\n","device: cpu\n"]}],"source":["import os\n","import json\n","import time\n","import random\n","from dataclasses import dataclass, asdict\n","from pathlib import Path\n","from typing import Callable, Dict, List, Tuple, Optional\n","\n","import numpy as np\n","import pandas as pd\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader, random_split\n","\n","import torchvision\n","import torchvision.transforms as T\n","\n","print(\"torch:\", torch.__version__)\n","print(\"torchvision:\", torchvision.__version__)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"device:\", device)"]},{"cell_type":"code","source":["# Inducing conflicting gradients (PCGrad projection with reversed criterion)\n","#\n","# Conditions:\n","# - **Control**: normal auxiliary-logit distillation (student matches teacher aux logits on noise).\n","# - **Intervention**: whenever alignment is **positive** (g_distill · g_trait > 0),\n","#   modify the distill gradient by projecting it onto the normal plane of g_trait:\n","#       g' = g_distill - proj_{g_trait}(g_distill)\n","#\n","# Measurements (logged every N steps):\n","# - trait loss (CE on first 10 logits, on audit batches)\n","# - distill loss (KL on aux logits, on current noise batch)\n","# - trait curvature along the *actual update direction* v (v^T H_trait v)\n","# - final performance (student test accuracy on MNIST)\n","# - For the **intervention** condition, the audit batches used to compute g_trait for projection at step k\n","#   are the **exact same audit batches** used to compute the logged trait loss/curvature at step k\n","#   (when step k is a logging step).\n","#\n","# Output:\n","# - **one CSV per (seed, condition)**: runs/.../seed_01/control_metrics.csv, intervention_metrics.csv\n","\n","@dataclass\n","class ExperimentConfig:\n","    # Runs\n","    seeds: List[int] = None # e.g., [1,2,...]\n","    out_dir: str = \"./runs_mnist_conflicting_grad\"\n","\n","    # Data\n","    batch_size: int = 1024\n","    num_workers: int = 0  # keep 0 for strict determinism\n","    audit_size: int = 10_000\n","    noise_dataset_size: int = 60_000\n","\n","    # Model (MLP from paper)\n","    hidden_dim: int = 256\n","    aux_m: int = 3\n","\n","    # Training\n","    teacher_epochs: int = 5\n","    student_epochs: int = 5\n","    lr_teacher: float = 3e-4\n","    lr_student: float = 3e-4\n","\n","    # Logging\n","    metrics_every_n_steps: int = 50\n","    # Trait batch size (USED BOTH for projection + for logged trait loss/curvature in intervention condition)\n","    # For control, it's only used at logging steps.\n","    audit_batches_for_trait: int = 1\n","\n","    # Numerics\n","    eps: float = 1e-12\n","\n","cfg = ExperimentConfig(\n","    seeds=list(range(1, 11)),\n","    out_dir=\"./runs_mnist_conflicting_grad\",\n","    batch_size=1024,\n","    num_workers=0,\n","    audit_size=10_000,\n","    noise_dataset_size=60_000,\n","    hidden_dim=256,\n","    aux_m=3,\n","    teacher_epochs=5,\n","    student_epochs=5,\n","    lr_teacher=3e-4,\n","    lr_student=3e-4,\n","    metrics_every_n_steps=10,\n","    audit_batches_for_trait=1,\n","    eps=1e-12,\n",")\n","\n","Path(cfg.out_dir).mkdir(parents=True, exist_ok=True)\n","cfg\n","\n","def set_global_seed(seed: int, deterministic: bool = True) -> None:\n","    \"\"\"Seed python, numpy, and torch. Optionally enable deterministic algorithms.\"\"\"\n","    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","\n","    if deterministic:\n","        torch.backends.cudnn.benchmark = False\n","        torch.backends.cudnn.deterministic = True\n","        try:\n","            torch.use_deterministic_algorithms(True)\n","        except Exception as e:\n","            print(\"Warning: could not enable full deterministic algorithms:\", e)\n","\n","def make_torch_generator(seed: int) -> torch.Generator:\n","    g = torch.Generator()\n","    g.manual_seed(seed)\n","    return g\n","\n","class NoiseImages(Dataset):\n","    \"\"\"Deterministic noise dataset: each index produces a reproducible noise image.\"\"\"\n","    def __init__(self, length: int, seed: int, shape=(1, 28, 28), dist: str = \"normal\"):\n","        self.length = int(length)\n","        self.seed = int(seed)\n","        self.shape = tuple(shape)\n","        self.dist = dist\n","\n","    def __len__(self) -> int:\n","        return self.length\n","\n","    def __getitem__(self, idx: int):\n","        # Per-index deterministic generation.\n","        g = torch.Generator()\n","        g.manual_seed(self.seed * 1_000_000 + int(idx))\n","        if self.dist == \"normal\":\n","            x = torch.randn(self.shape, generator=g)\n","        elif self.dist == \"uniform\":\n","            x = torch.rand(self.shape, generator=g) * 2 - 1\n","        else:\n","            raise ValueError(f\"Unknown dist: {self.dist}\")\n","        # Dummy label (unused)\n","        y = 0\n","        return x, y\n","\n","def get_mnist_datasets(root: str):\n","    transform = T.Compose([T.ToTensor()])\n","    train = torchvision.datasets.MNIST(root=root, train=True, download=True, transform=transform)\n","    test = torchvision.datasets.MNIST(root=root, train=False, download=True, transform=transform)\n","    return train, test\n","\n","def split_train_audit(train_ds, audit_size: int, seed: int):\n","    n = len(train_ds)\n","    audit_size = min(int(audit_size), n)\n","    train_size = n - audit_size\n","    g = make_torch_generator(seed)\n","    train_split, audit_split = random_split(train_ds, [train_size, audit_size], generator=g)\n","    return train_split, audit_split\n","\n","def make_loader(ds, batch_size: int, shuffle: bool, seed: int, num_workers: int = 0):\n","    # Deterministic shuffling via DataLoader generator.\n","    g = make_torch_generator(seed)\n","    return DataLoader(\n","        ds,\n","        batch_size=batch_size,\n","        shuffle=shuffle,\n","        num_workers=num_workers,\n","        pin_memory=torch.cuda.is_available(),\n","        generator=g,\n","        drop_last=False,\n","    )\n","\n","class MLPClassifier(nn.Module):\n","    \"\"\"MLP from the Subliminal Learning MNIST experiment: (784, 256, 256, 10+m) with ReLU.\"\"\"\n","    def __init__(self, hidden_dim: int = 256, aux_m: int = 3):\n","        super().__init__()\n","        self.hidden_dim = hidden_dim\n","        self.aux_m = aux_m\n","        self.fc1 = nn.Linear(28 * 28, hidden_dim)\n","        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n","        self.fc3 = nn.Linear(hidden_dim, 10 + aux_m)\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        x = x.view(x.size(0), -1)\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = self.fc3(x)\n","        return x\n","\n","def build_model_mlp(cfg: ExperimentConfig) -> nn.Module:\n","    \"\"\"Swap model architecture by changing this builder (or passing another builder to the runner).\"\"\"\n","    return MLPClassifier(hidden_dim=cfg.hidden_dim, aux_m=cfg.aux_m)\n","\n","def logits_regular(logits: torch.Tensor) -> torch.Tensor:\n","    return logits[:, :10]\n","\n","def logits_aux(logits: torch.Tensor, aux_m: int) -> torch.Tensor:\n","    return logits[:, 10:10 + aux_m]\n","\n","@torch.no_grad()\n","def accuracy_on_loader(model: nn.Module, loader: DataLoader, device: torch.device) -> float:\n","    model.eval()\n","    correct, total = 0, 0\n","    for x, y in loader:\n","        x, y = x.to(device), y.to(device)\n","        pred = logits_regular(model(x)).argmax(dim=1)\n","        correct += (pred == y).sum().item()\n","        total += y.numel()\n","    return correct / max(total, 1)\n","\n","def get_params(model: nn.Module) -> List[torch.nn.Parameter]:\n","    return [p for p in model.parameters() if p.requires_grad]\n","\n","def flatten_grads_from_params(params: List[torch.nn.Parameter]) -> torch.Tensor:\n","    \"\"\"Flatten gradients already stored in .grad (e.g., after backward()).\"\"\"\n","    flats = []\n","    for p in params:\n","        if p.grad is None:\n","            flats.append(torch.zeros_like(p).view(-1))\n","        else:\n","            flats.append(p.grad.detach().view(-1))\n","    return torch.cat(flats)\n","\n","def set_grads_from_flat(params: List[torch.nn.Parameter], flat: torch.Tensor) -> None:\n","    \"\"\"Overwrite params[i].grad with slices from flat.\"\"\"\n","    offset = 0\n","    for p in params:\n","        n = p.numel()\n","        g = flat[offset:offset + n].view_as(p).to(dtype=p.dtype, device=p.device)\n","        if p.grad is None:\n","            p.grad = g.clone()\n","        else:\n","            p.grad.copy_(g)\n","        offset += n\n","    assert offset == flat.numel()\n","\n","def split_flat_like_params(params: List[torch.nn.Parameter], flat: torch.Tensor) -> List[torch.Tensor]:\n","    out = []\n","    offset = 0\n","    for p in params:\n","        n = p.numel()\n","        out.append(flat[offset:offset + n].view_as(p))\n","        offset += n\n","    assert offset == flat.numel()\n","    return out\n","\n","def train_teacher(\n","    model: nn.Module,\n","    train_loader: DataLoader,\n","    test_loader: DataLoader,\n","    cfg: ExperimentConfig,\n","    device: torch.device,\n",") -> Dict[str, float]:\n","    \"\"\"Train teacher on MNIST CE using regular logits only.\"\"\"\n","    model = model.to(device)\n","    model.train()\n","    opt = torch.optim.Adam(model.parameters(), lr=cfg.lr_teacher)\n","\n","    for _epoch in range(cfg.teacher_epochs):\n","        for x, y in train_loader:\n","            x, y = x.to(device), y.to(device)\n","            opt.zero_grad(set_to_none=True)\n","            loss = F.cross_entropy(logits_regular(model(x)), y)\n","            loss.backward()\n","            opt.step()\n","\n","    teacher_acc = accuracy_on_loader(model, test_loader, device)\n","    return {\"teacher_test_acc\": float(teacher_acc)}\n","\n","def make_infinite_iterator(loader: DataLoader):\n","    it = iter(loader)\n","    while True:\n","        try:\n","            yield next(it)\n","        except StopIteration:\n","            it = iter(loader)\n","\n","@torch.no_grad()\n","def collect_audit_batches(audit_stream, num_batches: int) -> List[Tuple[torch.Tensor, torch.Tensor]]:\n","    return [next(audit_stream) for _ in range(num_batches)]\n","\n","def trait_loss_on_batches(\n","    model: nn.Module,\n","    batches: List[Tuple[torch.Tensor, torch.Tensor]],\n","    device: torch.device,\n",") -> torch.Tensor:\n","    losses = []\n","    for x, y in batches:\n","        x, y = x.to(device), y.to(device)\n","        losses.append(F.cross_entropy(logits_regular(model(x)), y))\n","    return torch.stack(losses).mean()\n","\n","def compute_trait_grad_flat_from_batches(\n","    model: nn.Module,\n","    params: List[torch.nn.Parameter],\n","    batches: List[Tuple[torch.Tensor, torch.Tensor]],\n","    device: torch.device,\n",") -> Tuple[float, torch.Tensor]:\n","    loss = trait_loss_on_batches(model, batches, device)\n","    grads = torch.autograd.grad(loss, params, retain_graph=False, create_graph=False)\n","    g_flat = torch.cat([g.detach().view(-1) for g in grads])\n","    return float(loss.detach()), g_flat\n","\n","def compute_trait_loss_and_curvature_vHv_from_batches(\n","    model: nn.Module,\n","    params: List[torch.nn.Parameter],\n","    batches: List[Tuple[torch.Tensor, torch.Tensor]],\n","    v_flat: torch.Tensor,\n","    device: torch.device,\n",") -> Tuple[float, float, float]:\n","    \"\"\"\n","    Returns:\n","      trait_loss, v^T H v, (v^T H v) / ||v||^2\n","    \"\"\"\n","    loss = trait_loss_on_batches(model, batches, device)\n","\n","    grads = torch.autograd.grad(loss, params, retain_graph=True, create_graph=True)\n","\n","    v_list = split_flat_like_params(params, v_flat.detach())\n","    gv = torch.zeros((), device=device)\n","    for g, v in zip(grads, v_list):\n","        gv = gv + (g * v).sum()\n","\n","    hvp = torch.autograd.grad(gv, params, retain_graph=False, create_graph=False)\n","    hvp_flat = torch.cat([h.detach().view(-1) for h in hvp])\n","\n","    v_det = v_flat.detach()\n","    vHv = float((v_det @ hvp_flat).detach())\n","    v_norm2 = float((v_det @ v_det).detach())\n","    vHv_norm = vHv / max(v_norm2, 1e-30)\n","\n","    return float(loss.detach()), vHv, vHv_norm\n","\n","def distill_loss_aux_only(student: nn.Module, teacher: nn.Module, x_noise: torch.Tensor, aux_m: int) -> torch.Tensor:\n","    with torch.no_grad():\n","        t_aux = logits_aux(teacher(x_noise), aux_m)\n","        t_prob = F.softmax(t_aux, dim=1)\n","\n","    s_aux = logits_aux(student(x_noise), aux_m)\n","    s_logprob = F.log_softmax(s_aux, dim=1)\n","\n","    kl = torch.nn.KLDivLoss(reduction=\"batchmean\")\n","    return kl(s_logprob, t_prob)\n","\n","# Intervention: PCGrad-style projection when dot(g_distill, g_trait) > 0\n","def project_distill_gradient_if_positive_alignment(\n","    g_distill: torch.Tensor,\n","    g_trait: torch.Tensor,\n","    eps: float = 1e-12,\n",") -> Tuple[torch.Tensor, bool, float]:\n","    dot = float((g_distill @ g_trait).detach())\n","    if dot > 0.0:\n","        denom = float((g_trait @ g_trait).detach()) + eps\n","        coeff = dot / denom\n","        g_update = g_distill - coeff * g_trait\n","        return g_update, True, dot\n","    return g_distill, False, dot\n","\n","# Student run (control vs intervention)\n","#\n","# Key detail:\n","# - In **intervention**, each step we draw `audit_batches_for_trait` audit batches once (call it `trait_batches_k`).\n","#   Those are used to compute g_trait for projection.\n","# - If this step is also a logging step, we compute trait loss + curvature using that **same** `trait_batches_k`.\n","#\n","def run_student_condition(\n","    condition: str,  # \"control\" or \"intervention\"\n","    student: nn.Module,\n","    teacher: nn.Module,\n","    noise_loader: DataLoader,\n","    audit_loader: DataLoader,\n","    test_loader: DataLoader,\n","    cfg: ExperimentConfig,\n","    seed: int,\n","    device: torch.device,\n",") -> Tuple[pd.DataFrame, Dict[str, float]]:\n","    assert condition in (\"control\", \"intervention\")\n","    assert cfg.audit_batches_for_trait >= 1\n","\n","    student = student.to(device)\n","    teacher = teacher.to(device)\n","    teacher.eval()\n","    for p in teacher.parameters():\n","        p.requires_grad_(False)\n","\n","    params = get_params(student)\n","    opt = torch.optim.Adam(student.parameters(), lr=cfg.lr_student)\n","\n","    audit_stream = make_infinite_iterator(audit_loader)\n","\n","    logs: List[Dict[str, float]] = []\n","    global_step = 0\n","    projected_steps = 0\n","\n","    student.train()\n","    for epoch in range(cfg.student_epochs):\n","        for x_noise, _ in noise_loader:\n","            x_noise = x_noise.to(device)\n","\n","            # --- distill gradient ---\n","            opt.zero_grad(set_to_none=True)\n","            dloss = distill_loss_aux_only(student, teacher, x_noise, cfg.aux_m)\n","            dloss.backward()\n","\n","            g_distill = flatten_grads_from_params(params)\n","            g_update = g_distill\n","            did_project = False\n","            dot_dt = float(\"nan\")\n","\n","            # We'll compute g_trait from trait_batches_k for:\n","            # - projection (intervention only, every step)\n","            # - logging (control and intervention, logging steps)\n","            trait_batches_k = None\n","            g_trait = None\n","\n","            # --- intervention projection gate (uses trait batches every step) ---\n","            if condition == \"intervention\":\n","                trait_batches_k = collect_audit_batches(audit_stream, cfg.audit_batches_for_trait)\n","                _trait_loss_tmp, g_trait = compute_trait_grad_flat_from_batches(\n","                    model=student,\n","                    params=params,\n","                    batches=trait_batches_k,\n","                    device=device,\n","                )\n","                g_update, did_project, dot_dt = project_distill_gradient_if_positive_alignment(\n","                    g_distill=g_distill,\n","                    g_trait=g_trait,\n","                    eps=cfg.eps,\n","                )\n","                if did_project:\n","                    projected_steps += 1\n","                    set_grads_from_flat(params, g_update)\n","\n","            # --- logging: trait loss + curvature + alignment (pre vs post) ---\n","            do_log = (global_step % cfg.metrics_every_n_steps == 0)\n","            if do_log:\n","                if condition == \"control\":\n","                    # Control: draw trait batches only at logging steps\n","                    trait_batches_k = collect_audit_batches(audit_stream, cfg.audit_batches_for_trait)\n","\n","                    # Need g_trait to compute inner/cos; compute it on *these same batches*\n","                    _trait_loss_tmp, g_trait = compute_trait_grad_flat_from_batches(\n","                        model=student,\n","                        params=params,\n","                        batches=trait_batches_k,\n","                        device=device,\n","                    )\n","                else:\n","                    # Intervention: trait_batches_k and g_trait already computed this step\n","                    assert trait_batches_k is not None and g_trait is not None\n","\n","                # Trait loss + curvature computed on the SAME batches.\n","                trait_loss_val, vHv, vHv_norm = compute_trait_loss_and_curvature_vHv_from_batches(\n","                    model=student,\n","                    params=params,\n","                    batches=trait_batches_k,\n","                    v_flat=g_update,  # curvature along actual update direction\n","                    device=device,\n","                )\n","\n","                # Alignment metrics: before vs after\n","                # pre: distill vs trait\n","                inner_pre = float((g_distill @ g_trait).detach())\n","                cos_pre = float((g_distill @ g_trait).detach() / ((g_distill.norm() * g_trait.norm()).clamp_min(cfg.eps)))\n","\n","                # post: update vs trait (same as pre for control; ~0 for projected steps)\n","                inner_post = float((g_update @ g_trait).detach())\n","                cos_post = float((g_update @ g_trait).detach() / ((g_update.norm() * g_trait.norm()).clamp_min(cfg.eps)))\n","\n","                logs.append({\n","                    \"seed\": seed,\n","                    \"condition\": condition,\n","                    \"step\": global_step,\n","                    \"epoch\": epoch,\n","\n","                    \"trait_loss\": trait_loss_val,\n","                    \"distill_loss\": float(dloss.detach()),\n","\n","                    \"trait_curvature_vHv\": vHv,\n","                    \"trait_curvature_vHv_norm\": vHv_norm,\n","\n","                    # alignment pre vs post\n","                    \"inner_pre\": inner_pre,\n","                    \"cos_pre\": cos_pre,\n","                    \"inner_post\": inner_post,\n","                    \"cos_post\": cos_post,\n","\n","                    # intervention diagnostics\n","                    \"did_project\": int(did_project) if condition == \"intervention\" else 0,\n","                    \"dot_gdistill_gtrait_step\": dot_dt if condition == \"intervention\" else np.nan,\n","                })\n","\n","            opt.step()\n","            global_step += 1\n","\n","    student_acc = accuracy_on_loader(student, test_loader, device)\n","\n","    df = pd.DataFrame(logs)\n","    info = {\n","        \"student_test_acc\": float(student_acc),\n","        \"total_steps\": int(global_step),\n","        \"num_logged_rows\": int(len(df)),\n","        \"projected_steps\": int(projected_steps),\n","        \"projected_fraction\": float(projected_steps / max(global_step, 1)),\n","    }\n","    return df, info\n","\n","# One seed: train teacher once, run control + intervention students\n","def run_one_seed(\n","    seed: int,\n","    cfg: ExperimentConfig,\n","    build_model_fn: Callable[[ExperimentConfig], nn.Module],\n","    device: torch.device,\n",") -> Dict[str, Path]:\n","    set_global_seed(seed, deterministic=True)\n","\n","    run_dir = Path(cfg.out_dir) / f\"seed_{seed:02d}\"\n","    run_dir.mkdir(parents=True, exist_ok=True)\n","\n","    # Data\n","    data_root = str(Path(cfg.out_dir) / \"data_cache\")\n","    mnist_train, mnist_test = get_mnist_datasets(root=data_root)\n","    train_split, audit_split = split_train_audit(mnist_train, audit_size=cfg.audit_size, seed=seed)\n","\n","    train_loader = make_loader(train_split, cfg.batch_size, shuffle=True,  seed=seed + 100, num_workers=cfg.num_workers)\n","    audit_loader = make_loader(audit_split, cfg.batch_size, shuffle=True,  seed=seed + 200, num_workers=cfg.num_workers)\n","    test_loader  = make_loader(mnist_test,  cfg.batch_size, shuffle=False, seed=seed + 300, num_workers=cfg.num_workers)\n","\n","    # Same noise ordering for both conditions\n","    noise_ds = NoiseImages(length=cfg.noise_dataset_size, seed=seed + 400, shape=(1, 28, 28), dist=\"normal\")\n","    noise_loader_control = make_loader(noise_ds, cfg.batch_size, shuffle=True, seed=seed + 500, num_workers=cfg.num_workers)\n","    noise_loader_interv  = make_loader(noise_ds, cfg.batch_size, shuffle=True, seed=seed + 500, num_workers=cfg.num_workers)\n","\n","    # Reference init\n","    reference = build_model_fn(cfg)\n","    ref_state = {k: v.clone().detach().cpu() for k, v in reference.state_dict().items()}\n","\n","    # Teacher\n","    teacher = build_model_fn(cfg)\n","    teacher.load_state_dict(ref_state)\n","    teacher_info = train_teacher(teacher, train_loader, test_loader, cfg, device)\n","\n","    # Students (same init)\n","    student_control = build_model_fn(cfg); student_control.load_state_dict(ref_state)\n","    student_interv  = build_model_fn(cfg); student_interv.load_state_dict(ref_state)\n","\n","    # Run conditions\n","    control_df, control_info = run_student_condition(\n","        condition=\"control\",\n","        student=student_control,\n","        teacher=teacher,\n","        noise_loader=noise_loader_control,\n","        audit_loader=audit_loader,\n","        test_loader=test_loader,\n","        cfg=cfg,\n","        seed=seed,\n","        device=device,\n","    )\n","    interv_df, interv_info = run_student_condition(\n","        condition=\"intervention\",\n","        student=student_interv,\n","        teacher=teacher,\n","        noise_loader=noise_loader_interv,\n","        audit_loader=audit_loader,\n","        test_loader=test_loader,\n","        cfg=cfg,\n","        seed=seed,\n","        device=device,\n","    )\n","\n","    # Add final perf to all rows\n","    for df, info in [(control_df, control_info), (interv_df, interv_info)]:\n","        df[\"teacher_test_acc\"] = teacher_info[\"teacher_test_acc\"]\n","        df[\"student_test_acc\"] = info[\"student_test_acc\"]\n","        df[\"total_steps\"] = info[\"total_steps\"]\n","        df[\"projected_steps\"] = info[\"projected_steps\"]\n","        df[\"projected_fraction\"] = info[\"projected_fraction\"]\n","\n","    # Save\n","    control_csv = run_dir / \"control_metrics.csv\"\n","    interv_csv  = run_dir / \"intervention_metrics.csv\"\n","    control_df.to_csv(control_csv, index=False)\n","    interv_df.to_csv(interv_csv, index=False)\n","\n","    meta = {\n","        \"seed\": seed,\n","        \"config\": asdict(cfg),\n","        \"teacher_info\": teacher_info,\n","        \"control_info\": control_info,\n","        \"intervention_info\": interv_info,\n","        \"created_at_utc\": time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime()),\n","        \"device\": str(device),\n","    }\n","    with open(run_dir / \"metadata.json\", \"w\") as f:\n","        json.dump(meta, f, indent=2)\n","\n","    print(\n","        f\"[seed {seed}] teacher_acc={teacher_info['teacher_test_acc']:.4f} | \"\n","        f\"control_acc={control_info['student_test_acc']:.4f} | \"\n","        f\"interv_acc={interv_info['student_test_acc']:.4f} | \"\n","        f\"proj_frac={interv_info['projected_fraction']:.3f} | \"\n","        f\"rows(control/interv)={len(control_df)}/{len(interv_df)}\"\n","    )\n","\n","    return {\"control_csv\": control_csv, \"intervention_csv\": interv_csv}\n","\n","# Run all seeds (1..10)\n","all_paths = []\n","for s in cfg.seeds:\n","    all_paths.append(run_one_seed(seed=s, cfg=cfg, build_model_fn=build_model_mlp, device=device))\n","\n","all_paths[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fHgQZVqknRJY","executionInfo":{"status":"ok","timestamp":1768766405914,"user_tz":-420,"elapsed":1240420,"user":{"displayName":"Chayanon Kitkana","userId":"07549743553674769868"}},"outputId":"9b522f89-b852-4239-b97c-1905e074fbc1"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 9.91M/9.91M [00:00<00:00, 130MB/s]\n","100%|██████████| 28.9k/28.9k [00:00<00:00, 19.7MB/s]\n","100%|██████████| 1.65M/1.65M [00:00<00:00, 72.4MB/s]\n","100%|██████████| 4.54k/4.54k [00:00<00:00, 7.62MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["[seed 1] teacher_acc=0.9296 | control_acc=0.1172 | interv_acc=0.1032 | proj_frac=0.542 | rows(control/interv)=30/30\n","[seed 2] teacher_acc=0.9276 | control_acc=0.1767 | interv_acc=0.1009 | proj_frac=0.590 | rows(control/interv)=30/30\n","[seed 3] teacher_acc=0.9315 | control_acc=0.3496 | interv_acc=0.1009 | proj_frac=0.539 | rows(control/interv)=30/30\n","[seed 4] teacher_acc=0.9291 | control_acc=0.1957 | interv_acc=0.1135 | proj_frac=0.542 | rows(control/interv)=30/30\n","[seed 5] teacher_acc=0.9295 | control_acc=0.4641 | interv_acc=0.1138 | proj_frac=0.559 | rows(control/interv)=30/30\n","[seed 6] teacher_acc=0.9323 | control_acc=0.3792 | interv_acc=0.0892 | proj_frac=0.505 | rows(control/interv)=30/30\n","[seed 7] teacher_acc=0.9332 | control_acc=0.3520 | interv_acc=0.1009 | proj_frac=0.583 | rows(control/interv)=30/30\n","[seed 8] teacher_acc=0.9316 | control_acc=0.2049 | interv_acc=0.0767 | proj_frac=0.569 | rows(control/interv)=30/30\n","[seed 9] teacher_acc=0.9305 | control_acc=0.3538 | interv_acc=0.0892 | proj_frac=0.492 | rows(control/interv)=30/30\n","[seed 10] teacher_acc=0.9302 | control_acc=0.1047 | interv_acc=0.0952 | proj_frac=0.522 | rows(control/interv)=30/30\n"]},{"output_type":"execute_result","data":{"text/plain":["{'control_csv': PosixPath('runs_mnist_conflicting_grad/seed_01/control_metrics.csv'),\n"," 'intervention_csv': PosixPath('runs_mnist_conflicting_grad/seed_01/intervention_metrics.csv')}"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["# Aggregate summary across runs\n","def aggregate_across_runs(out_dir: str) -> pd.DataFrame:\n","    out_dir = Path(out_dir)\n","    rows = []\n","    for seed_dir in sorted(out_dir.glob(\"seed_*\")):\n","        for cond, fname in [(\"control\", \"control_metrics.csv\"), (\"intervention\", \"intervention_metrics.csv\")]:\n","            p = seed_dir / fname\n","            if not p.exists():\n","                continue\n","\n","            df = pd.read_csv(p)\n","            if len(df) == 0:\n","                continue\n","\n","            # Helpers for safe means\n","            def _mean(col: str):\n","                return float(df[col].mean()) if col in df.columns else np.nan\n","\n","            rows.append({\n","                \"seed\": int(df[\"seed\"].iloc[0]),\n","                \"condition\": cond,\n","\n","                # performance\n","                \"teacher_test_acc\": float(df[\"teacher_test_acc\"].iloc[0]) if \"teacher_test_acc\" in df.columns else np.nan,\n","                \"student_test_acc\": float(df[\"student_test_acc\"].iloc[0]) if \"student_test_acc\" in df.columns else np.nan,\n","\n","                # losses\n","                \"mean_trait_loss\": _mean(\"trait_loss\"),\n","                \"mean_distill_loss\": _mean(\"distill_loss\"),\n","\n","                # curvature\n","                \"mean_trait_curvature_vHv\": _mean(\"trait_curvature_vHv\"),\n","                \"mean_trait_curvature_vHv_norm\": _mean(\"trait_curvature_vHv_norm\"),\n","\n","                # alignment summaries\n","                \"mean_inner_pre\": _mean(\"inner_pre\"),\n","                \"mean_cos_pre\": _mean(\"cos_pre\"),\n","                \"mean_inner_post\": _mean(\"inner_post\"),\n","                \"mean_cos_post\": _mean(\"cos_post\"),\n","\n","                # fractions of positive alignment (pre/post)\n","                \"frac_pos_pre\": float((df[\"inner_pre\"] > 0).mean()) if \"inner_pre\" in df.columns else np.nan,\n","                \"frac_pos_post\": float((df[\"inner_post\"] > 0).mean()) if \"inner_post\" in df.columns else np.nan,\n","\n","                # intervention stats (can be 0/NaN for control depending on the CSV)\n","                \"projected_fraction\": float(df[\"projected_fraction\"].iloc[0]) if \"projected_fraction\" in df.columns else np.nan,\n","                \"projected_steps\": int(df[\"projected_steps\"].iloc[0]) if \"projected_steps\" in df.columns else np.nan,\n","\n","                \"num_logged_rows\": int(len(df)),\n","            })\n","\n","    return pd.DataFrame(rows).sort_values([\"seed\", \"condition\"]).reset_index(drop=True)\n","\n","summary = aggregate_across_runs(cfg.out_dir)\n","summary_path = Path(cfg.out_dir) / \"summary_by_seed_condition.csv\"\n","summary.to_csv(summary_path, index=False)\n","summary"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":697},"id":"Wzv0EQqMnVLM","executionInfo":{"status":"ok","timestamp":1768766406067,"user_tz":-420,"elapsed":155,"user":{"displayName":"Chayanon Kitkana","userId":"07549743553674769868"}},"outputId":"34dea742-0d49-4e76-abc3-ec4349787b3e"},"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["    seed     condition  teacher_test_acc  student_test_acc  mean_trait_loss  \\\n","0      1       control            0.9296            0.1172         2.266872   \n","1      1  intervention            0.9296            0.1032         2.881673   \n","2      2       control            0.9276            0.1767         2.249522   \n","3      2  intervention            0.9276            0.1009         3.018697   \n","4      3       control            0.9315            0.3496         2.262300   \n","5      3  intervention            0.9315            0.1009         3.205902   \n","6      4       control            0.9291            0.1957         2.281864   \n","7      4  intervention            0.9291            0.1135         3.000659   \n","8      5       control            0.9295            0.4641         2.245749   \n","9      5  intervention            0.9295            0.1138         3.431964   \n","10     6       control            0.9323            0.3792         2.243400   \n","11     6  intervention            0.9323            0.0892         2.848000   \n","12     7       control            0.9332            0.3520         2.250862   \n","13     7  intervention            0.9332            0.1009         3.228510   \n","14     8       control            0.9316            0.2049         2.252716   \n","15     8  intervention            0.9316            0.0767         2.812215   \n","16     9       control            0.9305            0.3538         2.254666   \n","17     9  intervention            0.9305            0.0892         2.847154   \n","18    10       control            0.9302            0.1047         2.273381   \n","19    10  intervention            0.9302            0.0952         3.382699   \n","\n","    mean_distill_loss  mean_trait_curvature_vHv  \\\n","0            0.005360              3.015326e-06   \n","1            0.005389              2.776160e-06   \n","2            0.006357              4.175203e-06   \n","3            0.006366              4.182360e-06   \n","4            0.006645              3.738242e-06   \n","5            0.006650              3.510744e-06   \n","6            0.006152              5.368970e-06   \n","7            0.006176              4.737265e-06   \n","8            0.004986              6.885627e-07   \n","9            0.004997              4.936192e-07   \n","10           0.005431              8.959207e-07   \n","11           0.005441              1.264538e-06   \n","12           0.005148              2.423474e-06   \n","13           0.005165              2.567851e-06   \n","14           0.004314              1.476256e-06   \n","15           0.004328              1.384712e-06   \n","16           0.005744              1.244992e-06   \n","17           0.005755              1.293390e-06   \n","18           0.007198              2.541571e-06   \n","19           0.007206              3.078643e-06   \n","\n","    mean_trait_curvature_vHv_norm  mean_inner_pre  mean_cos_pre  \\\n","0                        0.001232        0.000074      0.008369   \n","1                       -0.001099        0.000547      0.013531   \n","2                        0.001606        0.000077      0.006263   \n","3                        0.002693       -0.000123     -0.002574   \n","4                        0.001343        0.000062      0.007039   \n","5                       -0.000105        0.000922      0.015501   \n","6                        0.001862        0.000037      0.005289   \n","7                       -0.000500        0.000521      0.013675   \n","8                        0.001375        0.000077      0.006763   \n","9                        0.001769        0.000405      0.006537   \n","10                       0.001108        0.000027      0.004714   \n","11                       0.002883       -0.000028      0.001720   \n","12                       0.001844        0.000063      0.007966   \n","13                       0.001911        0.000341      0.003340   \n","14                       0.001673        0.000038      0.005847   \n","15                       0.000284        0.000480      0.012142   \n","16                       0.001102        0.000061      0.005403   \n","17                       0.001806        0.000352      0.008055   \n","18                       0.002066        0.000049      0.004752   \n","19                       0.004698        0.000528      0.006983   \n","\n","    mean_inner_post  mean_cos_post  frac_pos_pre  frac_pos_post  \\\n","0          0.000074       0.008369      0.766667       0.766667   \n","1         -0.000135      -0.007071      0.566667       0.066667   \n","2          0.000077       0.006263      0.833333       0.833333   \n","3         -0.000398      -0.011120      0.433333       0.000000   \n","4          0.000062       0.007039      0.866667       0.866667   \n","5         -0.000223      -0.006574      0.666667       0.066667   \n","6          0.000037       0.005289      0.700000       0.700000   \n","7         -0.000492      -0.016196      0.600000       0.033333   \n","8          0.000077       0.006763      0.800000       0.800000   \n","9         -0.000631      -0.013592      0.533333       0.000000   \n","10         0.000027       0.004714      0.800000       0.800000   \n","11        -0.000283      -0.009062      0.366667       0.000000   \n","12         0.000063       0.007966      0.933333       0.933333   \n","13        -0.000346      -0.010465      0.566667       0.000000   \n","14         0.000038       0.005847      0.733333       0.733333   \n","15        -0.000119      -0.005846      0.600000       0.033333   \n","16         0.000061       0.005403      0.900000       0.900000   \n","17        -0.000231      -0.011214      0.500000       0.000000   \n","18         0.000049       0.004752      0.733333       0.733333   \n","19        -0.000369      -0.008779      0.600000       0.066667   \n","\n","    projected_fraction  projected_steps  num_logged_rows  \n","0             0.000000                0               30  \n","1             0.542373              160               30  \n","2             0.000000                0               30  \n","3             0.589831              174               30  \n","4             0.000000                0               30  \n","5             0.538983              159               30  \n","6             0.000000                0               30  \n","7             0.542373              160               30  \n","8             0.000000                0               30  \n","9             0.559322              165               30  \n","10            0.000000                0               30  \n","11            0.505085              149               30  \n","12            0.000000                0               30  \n","13            0.583051              172               30  \n","14            0.000000                0               30  \n","15            0.569492              168               30  \n","16            0.000000                0               30  \n","17            0.491525              145               30  \n","18            0.000000                0               30  \n","19            0.522034              154               30  "],"text/html":["\n","  <div id=\"df-48fc3417-3bdd-445c-ad47-5a913af9c685\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>seed</th>\n","      <th>condition</th>\n","      <th>teacher_test_acc</th>\n","      <th>student_test_acc</th>\n","      <th>mean_trait_loss</th>\n","      <th>mean_distill_loss</th>\n","      <th>mean_trait_curvature_vHv</th>\n","      <th>mean_trait_curvature_vHv_norm</th>\n","      <th>mean_inner_pre</th>\n","      <th>mean_cos_pre</th>\n","      <th>mean_inner_post</th>\n","      <th>mean_cos_post</th>\n","      <th>frac_pos_pre</th>\n","      <th>frac_pos_post</th>\n","      <th>projected_fraction</th>\n","      <th>projected_steps</th>\n","      <th>num_logged_rows</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>control</td>\n","      <td>0.9296</td>\n","      <td>0.1172</td>\n","      <td>2.266872</td>\n","      <td>0.005360</td>\n","      <td>3.015326e-06</td>\n","      <td>0.001232</td>\n","      <td>0.000074</td>\n","      <td>0.008369</td>\n","      <td>0.000074</td>\n","      <td>0.008369</td>\n","      <td>0.766667</td>\n","      <td>0.766667</td>\n","      <td>0.000000</td>\n","      <td>0</td>\n","      <td>30</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>intervention</td>\n","      <td>0.9296</td>\n","      <td>0.1032</td>\n","      <td>2.881673</td>\n","      <td>0.005389</td>\n","      <td>2.776160e-06</td>\n","      <td>-0.001099</td>\n","      <td>0.000547</td>\n","      <td>0.013531</td>\n","      <td>-0.000135</td>\n","      <td>-0.007071</td>\n","      <td>0.566667</td>\n","      <td>0.066667</td>\n","      <td>0.542373</td>\n","      <td>160</td>\n","      <td>30</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>control</td>\n","      <td>0.9276</td>\n","      <td>0.1767</td>\n","      <td>2.249522</td>\n","      <td>0.006357</td>\n","      <td>4.175203e-06</td>\n","      <td>0.001606</td>\n","      <td>0.000077</td>\n","      <td>0.006263</td>\n","      <td>0.000077</td>\n","      <td>0.006263</td>\n","      <td>0.833333</td>\n","      <td>0.833333</td>\n","      <td>0.000000</td>\n","      <td>0</td>\n","      <td>30</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2</td>\n","      <td>intervention</td>\n","      <td>0.9276</td>\n","      <td>0.1009</td>\n","      <td>3.018697</td>\n","      <td>0.006366</td>\n","      <td>4.182360e-06</td>\n","      <td>0.002693</td>\n","      <td>-0.000123</td>\n","      <td>-0.002574</td>\n","      <td>-0.000398</td>\n","      <td>-0.011120</td>\n","      <td>0.433333</td>\n","      <td>0.000000</td>\n","      <td>0.589831</td>\n","      <td>174</td>\n","      <td>30</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>3</td>\n","      <td>control</td>\n","      <td>0.9315</td>\n","      <td>0.3496</td>\n","      <td>2.262300</td>\n","      <td>0.006645</td>\n","      <td>3.738242e-06</td>\n","      <td>0.001343</td>\n","      <td>0.000062</td>\n","      <td>0.007039</td>\n","      <td>0.000062</td>\n","      <td>0.007039</td>\n","      <td>0.866667</td>\n","      <td>0.866667</td>\n","      <td>0.000000</td>\n","      <td>0</td>\n","      <td>30</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>3</td>\n","      <td>intervention</td>\n","      <td>0.9315</td>\n","      <td>0.1009</td>\n","      <td>3.205902</td>\n","      <td>0.006650</td>\n","      <td>3.510744e-06</td>\n","      <td>-0.000105</td>\n","      <td>0.000922</td>\n","      <td>0.015501</td>\n","      <td>-0.000223</td>\n","      <td>-0.006574</td>\n","      <td>0.666667</td>\n","      <td>0.066667</td>\n","      <td>0.538983</td>\n","      <td>159</td>\n","      <td>30</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>4</td>\n","      <td>control</td>\n","      <td>0.9291</td>\n","      <td>0.1957</td>\n","      <td>2.281864</td>\n","      <td>0.006152</td>\n","      <td>5.368970e-06</td>\n","      <td>0.001862</td>\n","      <td>0.000037</td>\n","      <td>0.005289</td>\n","      <td>0.000037</td>\n","      <td>0.005289</td>\n","      <td>0.700000</td>\n","      <td>0.700000</td>\n","      <td>0.000000</td>\n","      <td>0</td>\n","      <td>30</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>4</td>\n","      <td>intervention</td>\n","      <td>0.9291</td>\n","      <td>0.1135</td>\n","      <td>3.000659</td>\n","      <td>0.006176</td>\n","      <td>4.737265e-06</td>\n","      <td>-0.000500</td>\n","      <td>0.000521</td>\n","      <td>0.013675</td>\n","      <td>-0.000492</td>\n","      <td>-0.016196</td>\n","      <td>0.600000</td>\n","      <td>0.033333</td>\n","      <td>0.542373</td>\n","      <td>160</td>\n","      <td>30</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>5</td>\n","      <td>control</td>\n","      <td>0.9295</td>\n","      <td>0.4641</td>\n","      <td>2.245749</td>\n","      <td>0.004986</td>\n","      <td>6.885627e-07</td>\n","      <td>0.001375</td>\n","      <td>0.000077</td>\n","      <td>0.006763</td>\n","      <td>0.000077</td>\n","      <td>0.006763</td>\n","      <td>0.800000</td>\n","      <td>0.800000</td>\n","      <td>0.000000</td>\n","      <td>0</td>\n","      <td>30</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>5</td>\n","      <td>intervention</td>\n","      <td>0.9295</td>\n","      <td>0.1138</td>\n","      <td>3.431964</td>\n","      <td>0.004997</td>\n","      <td>4.936192e-07</td>\n","      <td>0.001769</td>\n","      <td>0.000405</td>\n","      <td>0.006537</td>\n","      <td>-0.000631</td>\n","      <td>-0.013592</td>\n","      <td>0.533333</td>\n","      <td>0.000000</td>\n","      <td>0.559322</td>\n","      <td>165</td>\n","      <td>30</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>6</td>\n","      <td>control</td>\n","      <td>0.9323</td>\n","      <td>0.3792</td>\n","      <td>2.243400</td>\n","      <td>0.005431</td>\n","      <td>8.959207e-07</td>\n","      <td>0.001108</td>\n","      <td>0.000027</td>\n","      <td>0.004714</td>\n","      <td>0.000027</td>\n","      <td>0.004714</td>\n","      <td>0.800000</td>\n","      <td>0.800000</td>\n","      <td>0.000000</td>\n","      <td>0</td>\n","      <td>30</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>6</td>\n","      <td>intervention</td>\n","      <td>0.9323</td>\n","      <td>0.0892</td>\n","      <td>2.848000</td>\n","      <td>0.005441</td>\n","      <td>1.264538e-06</td>\n","      <td>0.002883</td>\n","      <td>-0.000028</td>\n","      <td>0.001720</td>\n","      <td>-0.000283</td>\n","      <td>-0.009062</td>\n","      <td>0.366667</td>\n","      <td>0.000000</td>\n","      <td>0.505085</td>\n","      <td>149</td>\n","      <td>30</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>7</td>\n","      <td>control</td>\n","      <td>0.9332</td>\n","      <td>0.3520</td>\n","      <td>2.250862</td>\n","      <td>0.005148</td>\n","      <td>2.423474e-06</td>\n","      <td>0.001844</td>\n","      <td>0.000063</td>\n","      <td>0.007966</td>\n","      <td>0.000063</td>\n","      <td>0.007966</td>\n","      <td>0.933333</td>\n","      <td>0.933333</td>\n","      <td>0.000000</td>\n","      <td>0</td>\n","      <td>30</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>7</td>\n","      <td>intervention</td>\n","      <td>0.9332</td>\n","      <td>0.1009</td>\n","      <td>3.228510</td>\n","      <td>0.005165</td>\n","      <td>2.567851e-06</td>\n","      <td>0.001911</td>\n","      <td>0.000341</td>\n","      <td>0.003340</td>\n","      <td>-0.000346</td>\n","      <td>-0.010465</td>\n","      <td>0.566667</td>\n","      <td>0.000000</td>\n","      <td>0.583051</td>\n","      <td>172</td>\n","      <td>30</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>8</td>\n","      <td>control</td>\n","      <td>0.9316</td>\n","      <td>0.2049</td>\n","      <td>2.252716</td>\n","      <td>0.004314</td>\n","      <td>1.476256e-06</td>\n","      <td>0.001673</td>\n","      <td>0.000038</td>\n","      <td>0.005847</td>\n","      <td>0.000038</td>\n","      <td>0.005847</td>\n","      <td>0.733333</td>\n","      <td>0.733333</td>\n","      <td>0.000000</td>\n","      <td>0</td>\n","      <td>30</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>8</td>\n","      <td>intervention</td>\n","      <td>0.9316</td>\n","      <td>0.0767</td>\n","      <td>2.812215</td>\n","      <td>0.004328</td>\n","      <td>1.384712e-06</td>\n","      <td>0.000284</td>\n","      <td>0.000480</td>\n","      <td>0.012142</td>\n","      <td>-0.000119</td>\n","      <td>-0.005846</td>\n","      <td>0.600000</td>\n","      <td>0.033333</td>\n","      <td>0.569492</td>\n","      <td>168</td>\n","      <td>30</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>9</td>\n","      <td>control</td>\n","      <td>0.9305</td>\n","      <td>0.3538</td>\n","      <td>2.254666</td>\n","      <td>0.005744</td>\n","      <td>1.244992e-06</td>\n","      <td>0.001102</td>\n","      <td>0.000061</td>\n","      <td>0.005403</td>\n","      <td>0.000061</td>\n","      <td>0.005403</td>\n","      <td>0.900000</td>\n","      <td>0.900000</td>\n","      <td>0.000000</td>\n","      <td>0</td>\n","      <td>30</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>9</td>\n","      <td>intervention</td>\n","      <td>0.9305</td>\n","      <td>0.0892</td>\n","      <td>2.847154</td>\n","      <td>0.005755</td>\n","      <td>1.293390e-06</td>\n","      <td>0.001806</td>\n","      <td>0.000352</td>\n","      <td>0.008055</td>\n","      <td>-0.000231</td>\n","      <td>-0.011214</td>\n","      <td>0.500000</td>\n","      <td>0.000000</td>\n","      <td>0.491525</td>\n","      <td>145</td>\n","      <td>30</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>10</td>\n","      <td>control</td>\n","      <td>0.9302</td>\n","      <td>0.1047</td>\n","      <td>2.273381</td>\n","      <td>0.007198</td>\n","      <td>2.541571e-06</td>\n","      <td>0.002066</td>\n","      <td>0.000049</td>\n","      <td>0.004752</td>\n","      <td>0.000049</td>\n","      <td>0.004752</td>\n","      <td>0.733333</td>\n","      <td>0.733333</td>\n","      <td>0.000000</td>\n","      <td>0</td>\n","      <td>30</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>10</td>\n","      <td>intervention</td>\n","      <td>0.9302</td>\n","      <td>0.0952</td>\n","      <td>3.382699</td>\n","      <td>0.007206</td>\n","      <td>3.078643e-06</td>\n","      <td>0.004698</td>\n","      <td>0.000528</td>\n","      <td>0.006983</td>\n","      <td>-0.000369</td>\n","      <td>-0.008779</td>\n","      <td>0.600000</td>\n","      <td>0.066667</td>\n","      <td>0.522034</td>\n","      <td>154</td>\n","      <td>30</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-48fc3417-3bdd-445c-ad47-5a913af9c685')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-48fc3417-3bdd-445c-ad47-5a913af9c685 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-48fc3417-3bdd-445c-ad47-5a913af9c685');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","  <div id=\"id_78fb284a-77ba-4966-8244-a06b6f83a991\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('summary')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_78fb284a-77ba-4966-8244-a06b6f83a991 button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('summary');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"summary","summary":"{\n  \"name\": \"summary\",\n  \"rows\": 20,\n  \"fields\": [\n    {\n      \"column\": \"seed\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 1,\n        \"max\": 10,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          9,\n          2,\n          6\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"condition\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"intervention\",\n          \"control\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"teacher_test_acc\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0016225061835905266,\n        \"min\": 0.9276,\n        \"max\": 0.9332,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.9305,\n          0.9276\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"student_test_acc\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.1228890475184831,\n        \"min\": 0.0767,\n        \"max\": 0.4641,\n        \"num_unique_values\": 17,\n        \"samples\": [\n          0.1172,\n          0.1032\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mean_trait_loss\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.44385560365601656,\n        \"min\": 2.243400049209595,\n        \"max\": 3.431963912645976,\n        \"num_unique_values\": 20,\n        \"samples\": [\n          2.2668724854787192,\n          2.8471540371576944\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mean_distill_loss\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0008377719710988054,\n        \"min\": 0.004313785629346924,\n        \"max\": 0.007205800417189747,\n        \"num_unique_values\": 20,\n        \"samples\": [\n          0.005359914844545171,\n          0.005755349857887829\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mean_trait_curvature_vHv\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.4291892623702972e-06,\n        \"min\": 4.936191544402391e-07,\n        \"max\": 5.368970095679752e-06,\n        \"num_unique_values\": 20,\n        \"samples\": [\n          3.015326130177224e-06,\n          1.293390201695388e-06\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mean_trait_curvature_vHv_norm\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.001250881332775212,\n        \"min\": -0.0010989818935562903,\n        \"max\": 0.0046980614620456915,\n        \"num_unique_values\": 20,\n        \"samples\": [\n          0.0012320806984169868,\n          0.0018055764308581345\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mean_inner_pre\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.000268240768114819,\n        \"min\": -0.0001229528892205624,\n        \"max\": 0.0009216163090968549,\n        \"num_unique_values\": 20,\n        \"samples\": [\n          7.40464821319524e-05,\n          0.00035223125075086934\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mean_cos_pre\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.004233995954867652,\n        \"min\": -0.00257448534442421,\n        \"max\": 0.015500913683581507,\n        \"num_unique_values\": 20,\n        \"samples\": [\n          0.008368514868197945,\n          0.008054920565336939\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mean_inner_post\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.00022379789199563664,\n        \"min\": -0.0006313387877525896,\n        \"max\": 7.720243029324632e-05,\n        \"num_unique_values\": 20,\n        \"samples\": [\n          7.40464821319524e-05,\n          -0.00023123068538096064\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mean_cos_post\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.008664365067663207,\n        \"min\": -0.01619593447316073,\n        \"max\": 0.008368514868197945,\n        \"num_unique_values\": 20,\n        \"samples\": [\n          0.008368514868197945,\n          -0.01121432406076901\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"frac_pos_pre\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.15741887356472214,\n        \"min\": 0.36666666666666664,\n        \"max\": 0.9333333333333333,\n        \"num_unique_values\": 15,\n        \"samples\": [\n          0.5333333333333333,\n          0.9333333333333333\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"frac_pos_post\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.40414518843273806,\n        \"min\": 0.0,\n        \"max\": 0.9333333333333333,\n        \"num_unique_values\": 11,\n        \"samples\": [\n          0.7,\n          0.7666666666666667\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"projected_fraction\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.2801487380691215,\n        \"min\": 0.0,\n        \"max\": 0.5898305084745763,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.4915254237288136,\n          0.5423728813559322\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"projected_steps\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 82,\n        \"min\": 0,\n        \"max\": 174,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          145,\n          160\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"num_logged_rows\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 30,\n        \"max\": 30,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          30\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["!zip -r runs_mnist_conflicting_grad.zip ./runs_mnist_conflicting_grad"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Byi1oWdxnYAX","executionInfo":{"status":"ok","timestamp":1768766408545,"user_tz":-420,"elapsed":2469,"user":{"displayName":"Chayanon Kitkana","userId":"07549743553674769868"}},"outputId":"9bab9c49-8084-49c9-81f4-61a6e29e3f1f"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["  adding: runs_mnist_conflicting_grad/ (stored 0%)\n","  adding: runs_mnist_conflicting_grad/seed_03/ (stored 0%)\n","  adding: runs_mnist_conflicting_grad/seed_03/intervention_metrics.csv (deflated 65%)\n","  adding: runs_mnist_conflicting_grad/seed_03/metadata.json (deflated 59%)\n","  adding: runs_mnist_conflicting_grad/seed_03/control_metrics.csv (deflated 66%)\n","  adding: runs_mnist_conflicting_grad/seed_10/ (stored 0%)\n","  adding: runs_mnist_conflicting_grad/seed_10/intervention_metrics.csv (deflated 65%)\n","  adding: runs_mnist_conflicting_grad/seed_10/metadata.json (deflated 59%)\n","  adding: runs_mnist_conflicting_grad/seed_10/control_metrics.csv (deflated 66%)\n","  adding: runs_mnist_conflicting_grad/seed_06/ (stored 0%)\n","  adding: runs_mnist_conflicting_grad/seed_06/intervention_metrics.csv (deflated 67%)\n","  adding: runs_mnist_conflicting_grad/seed_06/metadata.json (deflated 59%)\n","  adding: runs_mnist_conflicting_grad/seed_06/control_metrics.csv (deflated 66%)\n","  adding: runs_mnist_conflicting_grad/summary_by_seed_condition.csv (deflated 60%)\n","  adding: runs_mnist_conflicting_grad/seed_08/ (stored 0%)\n","  adding: runs_mnist_conflicting_grad/seed_08/intervention_metrics.csv (deflated 65%)\n","  adding: runs_mnist_conflicting_grad/seed_08/metadata.json (deflated 59%)\n","  adding: runs_mnist_conflicting_grad/seed_08/control_metrics.csv (deflated 66%)\n","  adding: runs_mnist_conflicting_grad/seed_04/ (stored 0%)\n","  adding: runs_mnist_conflicting_grad/seed_04/intervention_metrics.csv (deflated 65%)\n","  adding: runs_mnist_conflicting_grad/seed_04/metadata.json (deflated 59%)\n","  adding: runs_mnist_conflicting_grad/seed_04/control_metrics.csv (deflated 66%)\n","  adding: runs_mnist_conflicting_grad/seed_05/ (stored 0%)\n","  adding: runs_mnist_conflicting_grad/seed_05/intervention_metrics.csv (deflated 66%)\n","  adding: runs_mnist_conflicting_grad/seed_05/metadata.json (deflated 59%)\n","  adding: runs_mnist_conflicting_grad/seed_05/control_metrics.csv (deflated 66%)\n","  adding: runs_mnist_conflicting_grad/data_cache/ (stored 0%)\n","  adding: runs_mnist_conflicting_grad/data_cache/MNIST/ (stored 0%)\n","  adding: runs_mnist_conflicting_grad/data_cache/MNIST/raw/ (stored 0%)\n","  adding: runs_mnist_conflicting_grad/data_cache/MNIST/raw/t10k-labels-idx1-ubyte (deflated 55%)\n","  adding: runs_mnist_conflicting_grad/data_cache/MNIST/raw/train-images-idx3-ubyte (deflated 79%)\n","  adding: runs_mnist_conflicting_grad/data_cache/MNIST/raw/train-images-idx3-ubyte.gz (deflated 0%)\n","  adding: runs_mnist_conflicting_grad/data_cache/MNIST/raw/train-labels-idx1-ubyte.gz (stored 0%)\n","  adding: runs_mnist_conflicting_grad/data_cache/MNIST/raw/t10k-images-idx3-ubyte (deflated 79%)\n","  adding: runs_mnist_conflicting_grad/data_cache/MNIST/raw/t10k-images-idx3-ubyte.gz (deflated 0%)\n","  adding: runs_mnist_conflicting_grad/data_cache/MNIST/raw/t10k-labels-idx1-ubyte.gz (stored 0%)\n","  adding: runs_mnist_conflicting_grad/data_cache/MNIST/raw/train-labels-idx1-ubyte (deflated 52%)\n","  adding: runs_mnist_conflicting_grad/seed_09/ (stored 0%)\n","  adding: runs_mnist_conflicting_grad/seed_09/intervention_metrics.csv (deflated 66%)\n","  adding: runs_mnist_conflicting_grad/seed_09/metadata.json (deflated 59%)\n","  adding: runs_mnist_conflicting_grad/seed_09/control_metrics.csv (deflated 66%)\n","  adding: runs_mnist_conflicting_grad/seed_01/ (stored 0%)\n","  adding: runs_mnist_conflicting_grad/seed_01/intervention_metrics.csv (deflated 65%)\n","  adding: runs_mnist_conflicting_grad/seed_01/metadata.json (deflated 59%)\n","  adding: runs_mnist_conflicting_grad/seed_01/control_metrics.csv (deflated 66%)\n","  adding: runs_mnist_conflicting_grad/seed_07/ (stored 0%)\n","  adding: runs_mnist_conflicting_grad/seed_07/intervention_metrics.csv (deflated 65%)\n","  adding: runs_mnist_conflicting_grad/seed_07/metadata.json (deflated 59%)\n","  adding: runs_mnist_conflicting_grad/seed_07/control_metrics.csv (deflated 66%)\n","  adding: runs_mnist_conflicting_grad/seed_02/ (stored 0%)\n","  adding: runs_mnist_conflicting_grad/seed_02/intervention_metrics.csv (deflated 66%)\n","  adding: runs_mnist_conflicting_grad/seed_02/metadata.json (deflated 59%)\n","  adding: runs_mnist_conflicting_grad/seed_02/control_metrics.csv (deflated 66%)\n"]}]}]}