{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1HPnoohuJD5Nt_CAUiGn5ax54Goe6YcIr","timestamp":1769006702201}],"authorship_tag":"ABX9TyPCIqog5ygskouAMCs4fMoU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_dcRz85rhl_n","executionInfo":{"status":"ok","timestamp":1770229384932,"user_tz":-420,"elapsed":13403,"user":{"displayName":"Chayanon Kitkana","userId":"07549743553674769868"}},"outputId":"a7fb8032-4ae3-4f5d-bbf8-a827d2632fbf"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch: 2.9.0+cpu\n","torchvision: 0.24.0+cpu\n","device: cpu\n"]}],"source":["import os\n","import math\n","import json\n","import time\n","import random\n","from dataclasses import dataclass, asdict\n","from pathlib import Path\n","from typing import Callable, Dict, List, Tuple, Optional\n","\n","import numpy as np\n","import pandas as pd\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader, random_split\n","\n","import torchvision\n","import torchvision.transforms as T\n","\n","print(\"torch:\", torch.__version__)\n","print(\"torchvision:\", torchvision.__version__)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"device:\", device)"]},{"cell_type":"code","source":["# MNIST auxiliary-logit distillation: gradient-alignment logging\n","#\n","# This notebook implements the MNIST MLP setup from *Subliminal Learning*:\n","# - Teacher: train on MNIST with CE using **first 10 logits only**.\n","# - Student: distill **aux logits only** (last `m` logits) from the teacher using KL on **noise images**.\n","#\n","# It logs (at a configurable interval) the **trait loss**, **distill loss**, **gradient inner product**, and\n","# **cosine similarity**, and saves **one CSV per seed**.\n","\n","@dataclass\n","class ExperimentConfig:\n","    # Runs\n","    seeds: List[int] = None  # e.g., [1,2,...]\n","    out_dir: str = \"./runs_mnist_subliminal_crt_period\"\n","\n","    # Data\n","    batch_size: int = 1024\n","    num_workers: int = 0  # keep 0 for strict determinism\n","    audit_size: int = 10_000\n","    noise_dataset_size: int = 60_000\n","\n","    # Model (MLP from paper)\n","    hidden_dim: int = 256\n","    aux_m: int = 3\n","\n","    # Training\n","    teacher_epochs: int = 5\n","    student_epochs: int = 5\n","    lr_teacher: float = 3e-4\n","    lr_student: float = 3e-4\n","\n","    # Logging\n","    metrics_every_n_steps: int = 50\n","    audit_batches_for_grad: Optional[int] = 1\n","    # If None, use the entire audit loader for trait gradients (slower but closer to full-set grads).\n","\n","    trait_acc_every_n_steps: int = 50\n","    trait_acc_max_batches: Optional[int] = 1\n","    # If None, use the entire like gradient.\n","\n","cfg = ExperimentConfig(\n","    seeds=list(range(1, 11)),\n","    out_dir=\"./runs_mnist_subliminal_crt_period\",\n","    batch_size=1024,\n","    num_workers=0,\n","    audit_size=10_000,\n","    noise_dataset_size=60_000,\n","    hidden_dim=256,\n","    aux_m=3,\n","    teacher_epochs=5,\n","    student_epochs=5,\n","    lr_teacher=3e-4,\n","    lr_student=3e-4,\n","    metrics_every_n_steps=1,\n","    audit_batches_for_grad=None,\n","    trait_acc_every_n_steps = 1,\n","    trait_acc_max_batches=None,\n",")\n","\n","Path(cfg.out_dir).mkdir(parents=True, exist_ok=True)\n","cfg\n","\n","def set_global_seed(seed: int, deterministic: bool = True) -> None:\n","    \"\"\"Seed python, numpy, and torch. Optionally enable deterministic algorithms.\"\"\"\n","    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","\n","    if deterministic:\n","        torch.backends.cudnn.benchmark = False\n","        torch.backends.cudnn.deterministic = True\n","        try:\n","            torch.use_deterministic_algorithms(True)\n","        except Exception as e:\n","            print(\"Warning: could not enable full deterministic algorithms:\", e)\n","\n","def make_torch_generator(seed: int) -> torch.Generator:\n","    g = torch.Generator()\n","    g.manual_seed(seed)\n","    return g\n","\n","class NoiseImages(Dataset):\n","    \"\"\"Deterministic noise dataset: each index produces a reproducible noise image.\"\"\"\n","    def __init__(self, length: int, seed: int, shape=(1, 28, 28), dist: str = \"normal\"):\n","        self.length = int(length)\n","        self.seed = int(seed)\n","        self.shape = tuple(shape)\n","        self.dist = dist\n","\n","    def __len__(self) -> int:\n","        return self.length\n","\n","    def __getitem__(self, idx: int):\n","        # Per-index deterministic generation.\n","        g = torch.Generator()\n","        g.manual_seed(self.seed * 1_000_000 + int(idx))\n","        if self.dist == \"normal\":\n","            x = torch.randn(self.shape, generator=g)\n","        elif self.dist == \"uniform\":\n","            x = torch.rand(self.shape, generator=g) * 2 - 1\n","        else:\n","            raise ValueError(f\"Unknown dist: {self.dist}\")\n","        # Dummy label (unused)\n","        y = 0\n","        return x, y\n","\n","def get_mnist_datasets(root: str = \"./data\"):\n","    transform = T.Compose([T.ToTensor()])\n","    train = torchvision.datasets.MNIST(root=root, train=True, download=True, transform=transform)\n","    test = torchvision.datasets.MNIST(root=root, train=False, download=True, transform=transform)\n","    return train, test\n","\n","def split_train_audit(train_ds, audit_size: int, seed: int):\n","    n = len(train_ds)\n","    audit_size = min(int(audit_size), n)\n","    train_size = n - audit_size\n","    g = make_torch_generator(seed)\n","    train_split, audit_split = random_split(train_ds, [train_size, audit_size], generator=g)\n","    return train_split, audit_split\n","\n","def make_loader(ds, batch_size: int, shuffle: bool, seed: int, num_workers: int = 0):\n","    # Deterministic shuffling via DataLoader generator.\n","    g = make_torch_generator(seed)\n","    return DataLoader(\n","        ds,\n","        batch_size=batch_size,\n","        shuffle=shuffle,\n","        num_workers=num_workers,\n","        pin_memory=torch.cuda.is_available(),\n","        generator=g,\n","        drop_last=False,\n","    )\n","\n","class MLPClassifier(nn.Module):\n","    \"\"\"MLP from the Subliminal Learning MNIST experiment: (784, 256, 256, 10+m) with ReLU.\"\"\"\n","    def __init__(self, hidden_dim: int = 256, aux_m: int = 3):\n","        super().__init__()\n","        self.hidden_dim = hidden_dim\n","        self.aux_m = aux_m\n","        self.fc1 = nn.Linear(28 * 28, hidden_dim)\n","        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n","        self.fc3 = nn.Linear(hidden_dim, 10 + aux_m)\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        x = x.view(x.size(0), -1)\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = self.fc3(x)\n","        return x\n","\n","def build_model_mlp(cfg: ExperimentConfig) -> nn.Module:\n","    \"\"\"Swap model architecture by changing this builder (or passing another builder to the runner).\"\"\"\n","    return MLPClassifier(hidden_dim=cfg.hidden_dim, aux_m=cfg.aux_m)\n","\n","def logits_regular(logits: torch.Tensor) -> torch.Tensor:\n","    return logits[:, :10]\n","\n","def logits_aux(logits: torch.Tensor, aux_m: int) -> torch.Tensor:\n","    return logits[:, 10:10 + aux_m]\n","\n","@torch.no_grad()\n","def accuracy_on_loader(model: nn.Module, loader: DataLoader, device: torch.device) -> float:\n","    model.eval()\n","    correct, total = 0, 0\n","    for x, y in loader:\n","        x = x.to(device)\n","        y = y.to(device)\n","        logits = logits_regular(model(x))\n","        pred = logits.argmax(dim=1)\n","        correct += (pred == y).sum().item()\n","        total += y.numel()\n","    return correct / max(total, 1)\n","\n","@torch.no_grad()\n","def accuracy_on_loader_e3(\n","    model: nn.Module,\n","    loader: DataLoader,\n","    device: torch.device,\n","    max_batches: Optional[int] = None,\n",") -> float:\n","    model.eval()\n","    correct, total = 0, 0\n","    for bi, (x,y)in enumerate(loader):\n","        if max_batches is not None and bi >= max_batches:\n","            break\n","        x = x.to(device)\n","        y = y.to(device)\n","        logits = logits_regular(model(x))\n","        pred = logits.argmax(dim=1)\n","        correct += (pred == y).sum().item()\n","        total += y.numel()\n","    return correct / max(total, 1)\n","\n","def flatten_grads_from_params(params: List[torch.nn.Parameter]) -> torch.Tensor:\n","    \"\"\"Flatten gradients already stored in .grad (e.g., after backward()).\"\"\"\n","    flats = []\n","    for p in params:\n","        if p.grad is None:\n","            flats.append(torch.zeros_like(p).view(-1))\n","        else:\n","            flats.append(p.grad.detach().view(-1))\n","    return torch.cat(flats)\n","\n","def flatten_grads_tuple(grads: Tuple[torch.Tensor, ...]) -> torch.Tensor:\n","    \"\"\"Flatten gradients returned by autograd.grad().\"\"\"\n","    flats = [g.detach().view(-1) for g in grads]\n","    return torch.cat(flats)\n","\n","def cosine_similarity(a: torch.Tensor, b: torch.Tensor, eps: float = 1e-12) -> float:\n","    denom = (a.norm() * b.norm()).clamp_min(eps)\n","    return float((a @ b) / denom)\n","\n","def train_teacher(\n","    model: nn.Module,\n","    train_loader: DataLoader,\n","    test_loader: DataLoader,\n","    cfg: ExperimentConfig,\n","    device: torch.device,\n",") -> Dict[str, float]:\n","    \"\"\"Train teacher on MNIST CE using regular logits only.\"\"\"\n","    model = model.to(device)\n","    model.train()\n","    opt = torch.optim.Adam(model.parameters(), lr=cfg.lr_teacher)\n","\n","    for epoch in range(cfg.teacher_epochs):\n","        for x, y in train_loader:\n","            x = x.to(device)\n","            y = y.to(device)\n","            opt.zero_grad(set_to_none=True)\n","            logits = logits_regular(model(x))\n","            loss = F.cross_entropy(logits, y)\n","            loss.backward()\n","            opt.step()\n","\n","    teacher_acc = accuracy_on_loader(model, test_loader, device)\n","    return {\"teacher_test_acc\": float(teacher_acc)}\n","\n","def compute_trait_loss_and_grad(\n","    student: nn.Module,\n","    audit_loader: DataLoader,\n","    cfg: ExperimentConfig,\n","    device: torch.device,\n",") -> Tuple[float, torch.Tensor]:\n","    \"\"\"\n","    Compute trait loss + gradient wrt student params over `audit_batches_for_grad` batches\n","    (or full loader if None).\n","    \"\"\"\n","    params = [p for p in student.parameters() if p.requires_grad]\n","\n","    # Accumulate a mean loss across selected batches; gradient taken from that mean.\n","    losses = []\n","    for bi, (x, y) in enumerate(audit_loader):\n","        if cfg.audit_batches_for_grad is not None and bi >= cfg.audit_batches_for_grad:\n","            break\n","        x = x.to(device)\n","        y = y.to(device)\n","        logits = logits_regular(student(x))\n","        loss = F.cross_entropy(logits, y)\n","        losses.append(loss)\n","\n","    if len(losses) == 0:\n","        raise RuntimeError(\"audit_loader produced no batches\")\n","\n","    mean_loss = torch.stack(losses).mean()\n","    grads = torch.autograd.grad(mean_loss, params, retain_graph=False, create_graph=False)\n","    g_flat = flatten_grads_tuple(grads)\n","    return float(mean_loss.detach()), g_flat\n","\n","def distill_student_with_logging(\n","    student: nn.Module,\n","    teacher: nn.Module,\n","    noise_loader: DataLoader,\n","    audit_loader: DataLoader,\n","    test_loader: DataLoader,\n","    cfg: ExperimentConfig,\n","    seed: int,\n","    device: torch.device,\n",") -> Tuple[pd.DataFrame, Dict[str, float]]:\n","    \"\"\"Train student on noise to match teacher aux logits. Log gradient alignment at intervals.\"\"\"\n","    student = student.to(device)\n","    teacher = teacher.to(device)\n","    teacher.eval()\n","    for p in teacher.parameters():\n","        p.requires_grad_(False)\n","\n","    opt = torch.optim.Adam(student.parameters(), lr=cfg.lr_student)\n","    kl = torch.nn.KLDivLoss(reduction=\"batchmean\")\n","\n","    logs: List[Dict[str, float]] = []\n","    global_step = 0\n","\n","    params = [p for p in student.parameters() if p.requires_grad]\n","\n","    student.train()\n","    for epoch in range(cfg.student_epochs):\n","        for x_noise, _ in noise_loader:\n","            x_noise = x_noise.to(device)\n","\n","            # --- Distillation loss (aux logits only) ---\n","            with torch.no_grad():\n","                t_logits = teacher(x_noise)\n","                t_aux = logits_aux(t_logits, cfg.aux_m)\n","                t_prob = F.softmax(t_aux, dim=1)\n","\n","            opt.zero_grad(set_to_none=True)\n","            s_logits = student(x_noise)\n","            s_aux = logits_aux(s_logits, cfg.aux_m)\n","            s_logprob = F.log_softmax(s_aux, dim=1)\n","            distill_loss = kl(s_logprob, t_prob)\n","            distill_loss.backward()\n","\n","            do_log = (global_step % cfg.metrics_every_n_steps == 0)\n","            if do_log:\n","                # Snapshot distill gradient from .grad\n","                g_distill = flatten_grads_from_params(params)\n","\n","                # Trait loss + gradient on audit set\n","                trait_loss_val, g_trait = compute_trait_loss_and_grad(\n","                    student=student,\n","                    audit_loader=audit_loader,\n","                    cfg=cfg,\n","                    device=device,\n","                )\n","\n","                trait_acc = None\n","                if (global_step % cfg.trait_acc_every_n_steps) == 0:\n","                    # intermediate eval() for acc, then return to train()\n","                    student.eval()\n","                    trait_acc = accuracy_on_loader_e3(\n","                        model=student,\n","                        loader=test_loader,\n","                        device=device,\n","                        max_batches=cfg.trait_acc_max_batches,\n","                    )\n","                    student.train()\n","\n","                inner = float(g_trait @ g_distill)\n","                cos = cosine_similarity(g_trait, g_distill)\n","\n","                logs.append({\n","                    \"seed\": seed,\n","                    \"step\": global_step,\n","                    \"epoch\": epoch,\n","                    \"trait_loss\": trait_loss_val,\n","                    \"distill_loss\": float(distill_loss.detach()),\n","                    \"inner_product\": inner,\n","                    \"cosine_similarity\": cos,\n","                    \"g_trait_norm\": float(g_trait.norm().detach()),\n","                    \"g_distill_norm\": float(g_distill.norm().detach()),\n","                    \"trait_test_acc_step\": (float(trait_acc) if trait_acc is not None else np.nan),\n","                })\n","\n","            opt.step()\n","            global_step += 1\n","\n","    # Final performance\n","    student_acc = accuracy_on_loader(student, test_loader, device)\n","\n","    df = pd.DataFrame(logs)\n","    info = {\n","        \"student_test_acc\": float(student_acc),\n","        \"total_steps\": int(global_step),\n","        \"num_logged_rows\": int(len(df)),\n","    }\n","    return df, info\n","\n","def run_one_instance(\n","    seed: int,\n","    cfg: ExperimentConfig,\n","    build_model_fn: Callable[[ExperimentConfig], nn.Module],\n","    device: torch.device,\n",") -> Path:\n","    \"\"\"\n","    One fully independent run:\n","      - builds reference init\n","      - trains teacher\n","      - distills student\n","      - writes CSV + metadata\n","    \"\"\"\n","    set_global_seed(seed, deterministic=True)\n","\n","    run_dir = Path(cfg.out_dir) / f\"seed_{seed:02d}\"\n","    run_dir.mkdir(parents=True, exist_ok=True)\n","\n","    # --- Data ---\n","    mnist_train, mnist_test = get_mnist_datasets(root=str(Path(cfg.out_dir) / \"data_cache\"))\n","    train_split, audit_split = split_train_audit(mnist_train, audit_size=cfg.audit_size, seed=seed)\n","\n","    train_loader = make_loader(train_split, cfg.batch_size, shuffle=True, seed=seed + 100, num_workers=cfg.num_workers)\n","    audit_loader = make_loader(audit_split, cfg.batch_size, shuffle=True, seed=seed + 200, num_workers=cfg.num_workers)\n","    test_loader  = make_loader(mnist_test, cfg.batch_size, shuffle=False, seed=seed + 300, num_workers=cfg.num_workers)\n","\n","    noise_ds = NoiseImages(length=cfg.noise_dataset_size, seed=seed + 400, shape=(1, 28, 28), dist=\"normal\")\n","    noise_loader = make_loader(noise_ds, cfg.batch_size, shuffle=True, seed=seed + 500, num_workers=cfg.num_workers)\n","\n","    # --- Models: reference init, teacher trained from it, student is a fresh copy of reference init ---\n","    reference = build_model_fn(cfg)\n","    reference_state = {k: v.clone().detach().cpu() for k, v in reference.state_dict().items()}\n","\n","    teacher = build_model_fn(cfg)\n","    teacher.load_state_dict(reference_state)\n","\n","    student = build_model_fn(cfg)\n","    student.load_state_dict(reference_state)\n","\n","    # --- Teacher training ---\n","    teacher_info = train_teacher(\n","        model=teacher,\n","        train_loader=train_loader,\n","        test_loader=test_loader,\n","        cfg=cfg,\n","        device=device,\n","    )\n","\n","    # --- Student distillation + logging ---\n","    df, student_info = distill_student_with_logging(\n","        student=student,\n","        teacher=teacher,\n","        noise_loader=noise_loader,\n","        audit_loader=audit_loader,\n","        test_loader=test_loader,\n","        cfg=cfg,\n","        seed=seed,\n","        device=device,\n","    )\n","\n","    # Add final performance columns to every row\n","    df[\"teacher_test_acc\"] = teacher_info[\"teacher_test_acc\"]\n","    df[\"student_test_acc\"] = student_info[\"student_test_acc\"]\n","    df[\"total_steps\"] = student_info[\"total_steps\"]\n","\n","    # Write CSV\n","    csv_path = run_dir / \"metrics.csv\"\n","    df.to_csv(csv_path, index=False)\n","\n","    # Write a metadata json for reproducibility\n","    meta = {\n","        \"seed\": seed,\n","        \"config\": asdict(cfg),\n","        \"teacher_info\": teacher_info,\n","        \"student_info\": student_info,\n","        \"created_at_utc\": time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime()),\n","        \"device\": str(device),\n","    }\n","    with open(run_dir / \"metadata.json\", \"w\") as f:\n","        json.dump(meta, f, indent=2)\n","\n","    print(f\"[seed {seed}] teacher_acc={teacher_info['teacher_test_acc']:.4f} \"\n","          f\"student_acc={student_info['student_test_acc']:.4f} rows={len(df)} \"\n","          f\"csv={csv_path}\")\n","    return csv_path\n","\n","# Run all instances (seeds 1..10 by default)\n","csv_paths = []\n","for s in cfg.seeds:\n","    csv_paths.append(run_one_instance(seed=s, cfg=cfg, build_model_fn=build_model_mlp, device=device))\n","\n","print(f\"Wrote {len(csv_paths)} CSVs under {cfg.out_dir}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EJwnBzXftSgu","executionInfo":{"status":"ok","timestamp":1769363241882,"user_tz":-420,"elapsed":6739369,"user":{"displayName":"Chayanon Kitkana","userId":"07549743553674769868"}},"outputId":"2e832507-f145-43a2-fc21-ece31c10bf36"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 9.91M/9.91M [00:01<00:00, 5.99MB/s]\n","100%|██████████| 28.9k/28.9k [00:00<00:00, 158kB/s]\n","100%|██████████| 1.65M/1.65M [00:01<00:00, 1.52MB/s]\n","100%|██████████| 4.54k/4.54k [00:00<00:00, 6.03MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["[seed 1] teacher_acc=0.9296 student_acc=0.1171 rows=295 csv=runs_mnist_subliminal_crt_period/seed_01/metrics.csv\n","[seed 2] teacher_acc=0.9276 student_acc=0.1760 rows=295 csv=runs_mnist_subliminal_crt_period/seed_02/metrics.csv\n","[seed 3] teacher_acc=0.9315 student_acc=0.3504 rows=295 csv=runs_mnist_subliminal_crt_period/seed_03/metrics.csv\n","[seed 4] teacher_acc=0.9291 student_acc=0.1962 rows=295 csv=runs_mnist_subliminal_crt_period/seed_04/metrics.csv\n","[seed 5] teacher_acc=0.9296 student_acc=0.4617 rows=295 csv=runs_mnist_subliminal_crt_period/seed_05/metrics.csv\n","[seed 6] teacher_acc=0.9322 student_acc=0.3791 rows=295 csv=runs_mnist_subliminal_crt_period/seed_06/metrics.csv\n","[seed 7] teacher_acc=0.9332 student_acc=0.3537 rows=295 csv=runs_mnist_subliminal_crt_period/seed_07/metrics.csv\n","[seed 8] teacher_acc=0.9318 student_acc=0.2055 rows=295 csv=runs_mnist_subliminal_crt_period/seed_08/metrics.csv\n","[seed 9] teacher_acc=0.9304 student_acc=0.3518 rows=295 csv=runs_mnist_subliminal_crt_period/seed_09/metrics.csv\n","[seed 10] teacher_acc=0.9303 student_acc=0.1048 rows=295 csv=runs_mnist_subliminal_crt_period/seed_10/metrics.csv\n","Wrote 10 CSVs under ./runs_mnist_subliminal_crt_period\n"]}]},{"cell_type":"code","source":["RUNS_DIR = Path(\"./runs_mnist_subliminal_crt_period\")\n","\n","def load_all_runs(runs_dir: Path) -> pd.DataFrame:\n","    \"\"\"Load all seed_*/metrics.csv into one dataframe.\"\"\"\n","    all_dfs = []\n","    for seed_dir in sorted(runs_dir.glob(\"seed_*\")):\n","        csv_path = seed_dir / \"metrics.csv\"\n","        if csv_path.exists():\n","            df = pd.read_csv(csv_path)\n","            df[\"run_dir\"] = str(seed_dir)\n","            all_dfs.append(df)\n","    return pd.concat(all_dfs, ignore_index=True)\n","\n","def compute_run_level_summary(df_all: pd.DataFrame) -> pd.DataFrame:\n","    \"\"\"\n","    Compute per-run (per-seed) metrics:\n","      - fraction of positive alignment (cos > 0)\n","      - mean cosine similarity\n","      - mean inner product\n","      - performance (student_test_acc, teacher_test_acc)\n","    \"\"\"\n","    def summarize_one_run(g: pd.DataFrame) -> pd.Series:\n","        cos = g[\"cosine_similarity\"].to_numpy()\n","        inner = g[\"inner_product\"].to_numpy()\n","\n","        frac_pos = float(np.mean(cos > 0)) if len(cos) else np.nan\n","        mean_cos = float(np.mean(cos)) if len(cos) else np.nan\n","        mean_inner = float(np.mean(inner)) if len(inner) else np.nan\n","\n","        # performance values are constant per run (we wrote them on every row)\n","        student_acc = float(g[\"student_test_acc\"].iloc[0]) if \"student_test_acc\" in g.columns else np.nan\n","        teacher_acc = float(g[\"teacher_test_acc\"].iloc[0]) if \"teacher_test_acc\" in g.columns else np.nan\n","\n","        return pd.Series({\n","            \"frac_positive_alignment\": frac_pos,\n","            \"mean_cosine_similarity\": mean_cos,\n","            \"mean_inner_product\": mean_inner,\n","            \"student_test_acc\": student_acc,\n","            \"teacher_test_acc\": teacher_acc,\n","            \"num_logged_rows\": int(len(g)),\n","        })\n","\n","    per_run = df_all.groupby(\"seed\", sort=True).apply(summarize_one_run).reset_index()\n","    return per_run\n","\n","def compute_overall_averages(per_run: pd.DataFrame) -> dict:\n","    \"\"\"Average across runs (seeds), each run weighted equally.\"\"\"\n","    keys = [\n","        \"frac_positive_alignment\",\n","        \"mean_cosine_similarity\",\n","        \"mean_inner_product\",\n","        \"student_test_acc\",\n","        \"teacher_test_acc\",\n","    ]\n","    out = {}\n","    for k in keys:\n","        out[k] = float(per_run[k].mean())\n","        out[k + \"_std\"] = float(per_run[k].std(ddof=1)) if len(per_run) > 1 else float(\"nan\")\n","    out[\"num_runs\"] = int(len(per_run))\n","    return out\n","\n","df_all = load_all_runs(RUNS_DIR)\n","per_run = compute_run_level_summary(df_all)\n","overall = compute_overall_averages(per_run)\n","\n","print(\"Per-run summary:\")\n","display(per_run)\n","\n","print(\"\\nOverall averages across runs (equal weight per seed):\")\n","for k, v in overall.items():\n","    print(f\"{k}: {v}\")\n","\n","per_run.to_csv(RUNS_DIR / \"per_run_summary.csv\", index=False)\n","pd.DataFrame([overall]).to_csv(RUNS_DIR / \"overall_summary.csv\", index=False)\n","print(f\"\\nSaved: {RUNS_DIR/'per_run_summary.csv'} and {RUNS_DIR/'overall_summary.csv'}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":695},"id":"2HrnB6FstXNE","executionInfo":{"status":"ok","timestamp":1769363342406,"user_tz":-420,"elapsed":104,"user":{"displayName":"Chayanon Kitkana","userId":"07549743553674769868"}},"outputId":"09222a30-d347-4f77-c5a0-06f2afff2224"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Per-run summary:\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-2431785010.py:43: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n","  per_run = df_all.groupby(\"seed\", sort=True).apply(summarize_one_run).reset_index()\n"]},{"output_type":"display_data","data":{"text/plain":["   seed  frac_positive_alignment  mean_cosine_similarity  mean_inner_product  \\\n","0     1                 0.722034                0.006719            0.000052   \n","1     2                 0.813559                0.007020            0.000055   \n","2     3                 0.847458                0.006026            0.000052   \n","3     4                 0.698305                0.005548            0.000034   \n","4     5                 0.759322                0.006611            0.000055   \n","5     6                 0.857627                0.007082            0.000058   \n","6     7                 0.928814                0.007763            0.000054   \n","7     8                 0.935593                0.006979            0.000044   \n","8     9                 0.905085                0.005819            0.000050   \n","9    10                 0.725424                0.004164            0.000048   \n","\n","   student_test_acc  teacher_test_acc  num_logged_rows  \n","0            0.1171            0.9296            295.0  \n","1            0.1760            0.9276            295.0  \n","2            0.3504            0.9315            295.0  \n","3            0.1962            0.9291            295.0  \n","4            0.4617            0.9296            295.0  \n","5            0.3791            0.9322            295.0  \n","6            0.3537            0.9332            295.0  \n","7            0.2055            0.9318            295.0  \n","8            0.3518            0.9304            295.0  \n","9            0.1048            0.9303            295.0  "],"text/html":["\n","  <div id=\"df-36e21651-6f0a-4404-8204-21d412edaa9a\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>seed</th>\n","      <th>frac_positive_alignment</th>\n","      <th>mean_cosine_similarity</th>\n","      <th>mean_inner_product</th>\n","      <th>student_test_acc</th>\n","      <th>teacher_test_acc</th>\n","      <th>num_logged_rows</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>0.722034</td>\n","      <td>0.006719</td>\n","      <td>0.000052</td>\n","      <td>0.1171</td>\n","      <td>0.9296</td>\n","      <td>295.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>0.813559</td>\n","      <td>0.007020</td>\n","      <td>0.000055</td>\n","      <td>0.1760</td>\n","      <td>0.9276</td>\n","      <td>295.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>0.847458</td>\n","      <td>0.006026</td>\n","      <td>0.000052</td>\n","      <td>0.3504</td>\n","      <td>0.9315</td>\n","      <td>295.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>0.698305</td>\n","      <td>0.005548</td>\n","      <td>0.000034</td>\n","      <td>0.1962</td>\n","      <td>0.9291</td>\n","      <td>295.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>0.759322</td>\n","      <td>0.006611</td>\n","      <td>0.000055</td>\n","      <td>0.4617</td>\n","      <td>0.9296</td>\n","      <td>295.0</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>6</td>\n","      <td>0.857627</td>\n","      <td>0.007082</td>\n","      <td>0.000058</td>\n","      <td>0.3791</td>\n","      <td>0.9322</td>\n","      <td>295.0</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>7</td>\n","      <td>0.928814</td>\n","      <td>0.007763</td>\n","      <td>0.000054</td>\n","      <td>0.3537</td>\n","      <td>0.9332</td>\n","      <td>295.0</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>8</td>\n","      <td>0.935593</td>\n","      <td>0.006979</td>\n","      <td>0.000044</td>\n","      <td>0.2055</td>\n","      <td>0.9318</td>\n","      <td>295.0</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>9</td>\n","      <td>0.905085</td>\n","      <td>0.005819</td>\n","      <td>0.000050</td>\n","      <td>0.3518</td>\n","      <td>0.9304</td>\n","      <td>295.0</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>10</td>\n","      <td>0.725424</td>\n","      <td>0.004164</td>\n","      <td>0.000048</td>\n","      <td>0.1048</td>\n","      <td>0.9303</td>\n","      <td>295.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-36e21651-6f0a-4404-8204-21d412edaa9a')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-36e21651-6f0a-4404-8204-21d412edaa9a button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-36e21651-6f0a-4404-8204-21d412edaa9a');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","  <div id=\"id_52571275-8bb5-45bf-8288-f3b4b0f03aa5\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('per_run')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_52571275-8bb5-45bf-8288-f3b4b0f03aa5 button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('per_run');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"per_run","summary":"{\n  \"name\": \"per_run\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"seed\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3,\n        \"min\": 1,\n        \"max\": 10,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          9,\n          2,\n          6\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"frac_positive_alignment\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.08924479484400052,\n        \"min\": 0.6983050847457627,\n        \"max\": 0.9355932203389831,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.9050847457627119,\n          0.8135593220338984,\n          0.8576271186440678\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mean_cosine_similarity\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0010212003111252238,\n        \"min\": 0.0041638372608774005,\n        \"max\": 0.007762861791013385,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.005819270051919553,\n          0.007020262237495154,\n          0.007082162747333201\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mean_inner_product\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 6.950832605318827e-06,\n        \"min\": 3.398234378405948e-05,\n        \"max\": 5.830613146548806e-05,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          4.9637140062840116e-05,\n          5.497425094852456e-05,\n          5.830613146548806e-05\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"student_test_acc\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.12379353824457523,\n        \"min\": 0.1048,\n        \"max\": 0.4617,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.3518,\n          0.176,\n          0.3791\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"teacher_test_acc\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0016633633931819594,\n        \"min\": 0.9276,\n        \"max\": 0.9332,\n        \"num_unique_values\": 9,\n        \"samples\": [\n          0.9304,\n          0.9276,\n          0.9332\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"num_logged_rows\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 295.0,\n        \"max\": 295.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          295.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Overall averages across runs (equal weight per seed):\n","frac_positive_alignment: 0.8193220338983049\n","frac_positive_alignment_std: 0.08924479484400052\n","mean_cosine_similarity: 0.006373161120200753\n","mean_cosine_similarity_std: 0.0010212003111252238\n","mean_inner_product: 5.018745162753113e-05\n","mean_inner_product_std: 6.950832605318827e-06\n","student_test_acc: 0.26963\n","student_test_acc_std: 0.12379353824457523\n","teacher_test_acc: 0.9305300000000001\n","teacher_test_acc_std: 0.0016633633931819594\n","num_runs: 10\n","\n","Saved: runs_mnist_subliminal_crt_period/per_run_summary.csv and runs_mnist_subliminal_crt_period/overall_summary.csv\n"]}]},{"cell_type":"code","source":["!zip -r runs_mnist_subliminal_crt_period.zip ./runs_mnist_subliminal_crt_period/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qvMagFCx-hLO","executionInfo":{"status":"ok","timestamp":1769363613803,"user_tz":-420,"elapsed":1915,"user":{"displayName":"Chayanon Kitkana","userId":"07549743553674769868"}},"outputId":"9767d7a9-6035-48ef-af74-0a905137403a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["  adding: runs_mnist_subliminal_crt_period/ (stored 0%)\n","  adding: runs_mnist_subliminal_crt_period/seed_06/ (stored 0%)\n","  adding: runs_mnist_subliminal_crt_period/seed_06/metadata.json (deflated 55%)\n","  adding: runs_mnist_subliminal_crt_period/seed_06/metrics.csv (deflated 59%)\n","  adding: runs_mnist_subliminal_crt_period/seed_02/ (stored 0%)\n","  adding: runs_mnist_subliminal_crt_period/seed_02/metadata.json (deflated 55%)\n","  adding: runs_mnist_subliminal_crt_period/seed_02/metrics.csv (deflated 59%)\n","  adding: runs_mnist_subliminal_crt_period/data_cache/ (stored 0%)\n","  adding: runs_mnist_subliminal_crt_period/data_cache/MNIST/ (stored 0%)\n","  adding: runs_mnist_subliminal_crt_period/data_cache/MNIST/raw/ (stored 0%)\n","  adding: runs_mnist_subliminal_crt_period/data_cache/MNIST/raw/t10k-labels-idx1-ubyte.gz (stored 0%)\n","  adding: runs_mnist_subliminal_crt_period/data_cache/MNIST/raw/t10k-images-idx3-ubyte (deflated 79%)\n","  adding: runs_mnist_subliminal_crt_period/data_cache/MNIST/raw/train-labels-idx1-ubyte.gz (stored 0%)\n","  adding: runs_mnist_subliminal_crt_period/data_cache/MNIST/raw/train-images-idx3-ubyte (deflated 79%)\n","  adding: runs_mnist_subliminal_crt_period/data_cache/MNIST/raw/t10k-images-idx3-ubyte.gz (deflated 0%)\n","  adding: runs_mnist_subliminal_crt_period/data_cache/MNIST/raw/train-labels-idx1-ubyte (deflated 52%)\n","  adding: runs_mnist_subliminal_crt_period/data_cache/MNIST/raw/t10k-labels-idx1-ubyte (deflated 55%)\n","  adding: runs_mnist_subliminal_crt_period/data_cache/MNIST/raw/train-images-idx3-ubyte.gz (deflated 0%)\n","  adding: runs_mnist_subliminal_crt_period/seed_03/ (stored 0%)\n","  adding: runs_mnist_subliminal_crt_period/seed_03/metadata.json (deflated 55%)\n","  adding: runs_mnist_subliminal_crt_period/seed_03/metrics.csv (deflated 58%)\n","  adding: runs_mnist_subliminal_crt_period/overall_summary.csv (deflated 48%)\n","  adding: runs_mnist_subliminal_crt_period/seed_08/ (stored 0%)\n","  adding: runs_mnist_subliminal_crt_period/seed_08/metadata.json (deflated 55%)\n","  adding: runs_mnist_subliminal_crt_period/seed_08/metrics.csv (deflated 59%)\n","  adding: runs_mnist_subliminal_crt_period/seed_01/ (stored 0%)\n","  adding: runs_mnist_subliminal_crt_period/seed_01/metadata.json (deflated 55%)\n","  adding: runs_mnist_subliminal_crt_period/seed_01/metrics.csv (deflated 59%)\n","  adding: runs_mnist_subliminal_crt_period/seed_04/ (stored 0%)\n","  adding: runs_mnist_subliminal_crt_period/seed_04/metadata.json (deflated 55%)\n","  adding: runs_mnist_subliminal_crt_period/seed_04/metrics.csv (deflated 59%)\n","  adding: runs_mnist_subliminal_crt_period/seed_07/ (stored 0%)\n","  adding: runs_mnist_subliminal_crt_period/seed_07/metadata.json (deflated 55%)\n","  adding: runs_mnist_subliminal_crt_period/seed_07/metrics.csv (deflated 59%)\n","  adding: runs_mnist_subliminal_crt_period/seed_05/ (stored 0%)\n","  adding: runs_mnist_subliminal_crt_period/seed_05/metadata.json (deflated 55%)\n","  adding: runs_mnist_subliminal_crt_period/seed_05/metrics.csv (deflated 58%)\n","  adding: runs_mnist_subliminal_crt_period/seed_10/ (stored 0%)\n","  adding: runs_mnist_subliminal_crt_period/seed_10/metadata.json (deflated 55%)\n","  adding: runs_mnist_subliminal_crt_period/seed_10/metrics.csv (deflated 60%)\n","  adding: runs_mnist_subliminal_crt_period/per_run_summary.csv (deflated 51%)\n","  adding: runs_mnist_subliminal_crt_period/seed_09/ (stored 0%)\n","  adding: runs_mnist_subliminal_crt_period/seed_09/metadata.json (deflated 55%)\n","  adding: runs_mnist_subliminal_crt_period/seed_09/metrics.csv (deflated 59%)\n"]}]}]}
